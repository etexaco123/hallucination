{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/s3886026/NLP_thesis/JoeyNMT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
    "# These will also become the suffix's of all vocab and corpus files used throughout\n",
    "source_language = \"de\"\n",
    "target_language = \"en\" \n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
    "\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"./$src-$tgt-$tag\"\n",
    "os.environ[\"env_path\"] = \"./%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999 ./out-domain/JRC/JRC6.clean.de\r\n",
      "99999 ./out-domain/JRC/JRC6.clean.en\r\n"
     ]
    }
   ],
   "source": [
    "# specify the file paths here\n",
    "# source_file = \"./corpus/EMEA/MOSES/EMEA.clean.de\"\n",
    "# target_file = \"./corpus/EMEA/MOSES/EMEA.clean.en\"\n",
    "source_file = \"./out-domain/JRC/JRC6.clean.de\"\n",
    "target_file = \"./out-domain/JRC/JRC6.clean.en\"\n",
    "\n",
    "# They should both have the same length.\n",
    "! wc -l \"$source_file\"\n",
    "! wc -l \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./out-domain/JRC/JRC6.clean.de <==\r\n",
      "zur anpassung der verordnung eg nr aufgrund des beitritts der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen union\r\n",
      "die kommission der europäischen gemeinschaften \r\n",
      "gestützt auf den vertrag zur gründung der europäischen gemeinschaft\r\n",
      "gestützt auf den vertrag über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz \r\n",
      "gestützt auf die akte über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz \r\n",
      "der beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen gemeinschaft am maerfordert eine sprachliche anpassung der verordnung eg nr der kommission vom april mit durchführungsbestimmungen zur verordnung eg nr des rates hinsichtlich des schutzes lebender rinder beim transport als voraussetzung für die gewährung von ausfuhrerstattungen\r\n",
      "die verordnung eg nr ist entsprechend zu ändern \r\n",
      "hat folgende verordnung erlassen\r\n",
      "artikel absatz der verordnung eg nr erhält folgende fassung\r\n",
      "stellt der amtliche tierarzt an der ausgangsstelle fest dass die voraussetzungen nach absatz erfuellt sind so bestätigt er dies durch einen der folgenden vermerke\r\n",
      "und durch seinen stempel und seine unterschrift im dokument über das verlassen des zollgebiets der gemeinschaft entweder in feld j des kontrollexemplars t oder an geeigneter stelle in der einzelstaatlichen bescheinigung\r\n",
      "diese verordnung tritt vorbehaltlich des inkrafttretens des vertrags über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeund zum zeitpunkt seines inkrafttretens in kraft\r\n",
      "diese verordnung ist in allen ihren teilen verbindlich und gilt unmittelbar in jedem mitgliedstaat\r\n",
      "verordnung eg nr der kommission\r\n",
      "zur änderung der verordnung eg nr mit sondermaßnahmen zur anpassung der modalitäten für die verwaltung der zollkontingente für die einfuhr von bananen infolge des beitritts der neuen mitgliedstaaten am ma\r\n",
      "\r\n",
      "==> ./out-domain/JRC/JRC6.clean.en <==\r\n",
      "adapting regulation ec no by reason of the accession to the european union of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia\r\n",
      "the commission of the european communities\r\n",
      "having regard to the treaty establishing the european community\r\n",
      "having regard to the treaty of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "having regard to the act of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "in view of the accession to the community on may of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia linguistic amendments need to be made to commission regulation ec no of april laying down detailed rules pursuant to council regulation ec no as regards requirements for the granting of export refunds related to the welfare of live bovine animals during transport\r\n",
      "regulation ec no should therefore be amended accordingly\r\n",
      "has adopted this regulation\r\n",
      "article of regulation ec no is hereby replaced by the following\r\n",
      "if the official veterinarian at the exit point is satisfied that the requirements of paragraph are met he shall certify this by one of the following entries\r\n",
      "and by stamping and signing the document constituting evidence of exit from the customs territory of the community either in section j of the control copy t or in the most appropriate place on the national document\r\n",
      "this regulation shall enter into force subject to and on the date of the entry into force of the treaty of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia\r\n",
      "this regulation shall be binding in its entirety and directly applicable in all member states\r\n",
      "commission regulation ec no \r\n",
      "amending regulation ec no adopting specific measures with a view to adapting the arrangements for administering tariff quotas on banana imports as a result of the accession of new member states on may \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 \"$source_file\" \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (0.0.49)\r\n",
      "Requirement already satisfied: joblib in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (1.1.0)\r\n",
      "Requirement already satisfied: click in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (7.1.2)\r\n",
      "Requirement already satisfied: regex in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (2022.3.15)\r\n",
      "Requirement already satisfied: tqdm in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (4.64.0)\r\n",
      "Requirement already satisfied: six in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (1.16.0)\r\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\r\n",
      "You should consider upgrading via the '/data/s3886026/envs/jnmt_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is need for tokenization, thise can be done via sacremoses\n",
    "tok_source_file = source_file+\".tok\"\n",
    "tok_target_file = target_file+\".tok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the source\n",
    "!sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
    "# Tokenize the target\n",
    "!sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./out-domain/JRC/JRC6.clean.de <==\r\n",
      "zur anpassung der verordnung eg nr aufgrund des beitritts der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen union\r\n",
      "die kommission der europäischen gemeinschaften \r\n",
      "gestützt auf den vertrag zur gründung der europäischen gemeinschaft\r\n",
      "gestützt auf den vertrag über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz \r\n",
      "gestützt auf die akte über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz \r\n",
      "der beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen gemeinschaft am maerfordert eine sprachliche anpassung der verordnung eg nr der kommission vom april mit durchführungsbestimmungen zur verordnung eg nr des rates hinsichtlich des schutzes lebender rinder beim transport als voraussetzung für die gewährung von ausfuhrerstattungen\r\n",
      "die verordnung eg nr ist entsprechend zu ändern \r\n",
      "hat folgende verordnung erlassen\r\n",
      "artikel absatz der verordnung eg nr erhält folgende fassung\r\n",
      "stellt der amtliche tierarzt an der ausgangsstelle fest dass die voraussetzungen nach absatz erfuellt sind so bestätigt er dies durch einen der folgenden vermerke\r\n",
      "\r\n",
      "==> ./out-domain/JRC/JRC6.clean.de.tok <==\r\n",
      "zur anpassung der verordnung eg nr aufgrund des beitritts der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen union\r\n",
      "die kommission der europäischen gemeinschaften\r\n",
      "gestützt auf den vertrag zur gründung der europäischen gemeinschaft\r\n",
      "gestützt auf den vertrag über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz\r\n",
      "gestützt auf die akte über den beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakeinsbesondere auf artikel absatz\r\n",
      "der beitritt der tschechischen republik estlands zyperns lettlands litauens ungarns maltas polens sloweniens und der slowakezur europäischen gemeinschaft am maerfordert eine sprachliche anpassung der verordnung eg nr der kommission vom april mit durchführungsbestimmungen zur verordnung eg nr des rates hinsichtlich des schutzes lebender rinder beim transport als voraussetzung für die gewährung von ausfuhrerstattungen\r\n",
      "die verordnung eg nr ist entsprechend zu ändern\r\n",
      "hat folgende verordnung erlassen\r\n",
      "artikel absatz der verordnung eg nr erhält folgende fassung\r\n",
      "stellt der amtliche tierarzt an der ausgangsstelle fest dass die voraussetzungen nach absatz erfuellt sind so bestätigt er dies durch einen der folgenden vermerke\r\n",
      "==> ./out-domain/JRC/JRC6.clean.en <==\r\n",
      "adapting regulation ec no by reason of the accession to the european union of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia\r\n",
      "the commission of the european communities\r\n",
      "having regard to the treaty establishing the european community\r\n",
      "having regard to the treaty of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "having regard to the act of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "in view of the accession to the community on may of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia linguistic amendments need to be made to commission regulation ec no of april laying down detailed rules pursuant to council regulation ec no as regards requirements for the granting of export refunds related to the welfare of live bovine animals during transport\r\n",
      "regulation ec no should therefore be amended accordingly\r\n",
      "has adopted this regulation\r\n",
      "article of regulation ec no is hereby replaced by the following\r\n",
      "if the official veterinarian at the exit point is satisfied that the requirements of paragraph are met he shall certify this by one of the following entries\r\n",
      "\r\n",
      "==> ./out-domain/JRC/JRC6.clean.en.tok <==\r\n",
      "adapting regulation ec no by reason of the accession to the european union of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia\r\n",
      "the commission of the european communities\r\n",
      "having regard to the treaty establishing the european community\r\n",
      "having regard to the treaty of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "having regard to the act of accession of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia and in particular article thereof\r\n",
      "in view of the accession to the community on may of the czech republic estonia cyprus latvia lithuania hungary malta poland slovenia and slovakia linguistic amendments need to be made to commission regulation ec no of april laying down detailed rules pursuant to council regulation ec no as regards requirements for the granting of export refunds related to the welfare of live bovine animals during transport\r\n",
      "regulation ec no should therefore be amended accordingly\r\n",
      "has adopted this regulation\r\n",
      "article of regulation ec no is hereby replaced by the following\r\n",
      "if the official veterinarian at the exit point is satisfied that the requirements of paragraph are met he shall certify this by one of the following entries\r\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look what tokenization did to the text.\n",
    "! head \"$source_file\"*\n",
    "! head \"$target_file\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the pointers to our files such that we continue to work with the tokenized data.\n",
    "source_file = tok_source_file\n",
    "target_file = tok_target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p /de-en-baseline/data\n",
    "#\"/home/s3886026/NLP_thesis/JoeyNMT/translate.ipynb\"\n",
    "#os.environ[\"DIR\"] = \"/home/s3886026/NLP_thesis/JoeyNMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 900000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 900000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 7000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 7000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 910000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 910000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 8000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 8000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: write error\r\n",
      "tail: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "!head -n 86000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "!head -n 86000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "!tail -n 4000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "!tail -n 4000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "!tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "!tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 776126 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 776126 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 332625 \"$source_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 332625 \"$target_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 110875 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 110875 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   86000  1839749 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.bpe.de\r\n",
      "   86000  1983074 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.bpe.en\r\n",
      "   86000  1719669 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\r\n",
      "   86000  1940855 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\r\n",
      "    2000    41701 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.bpe.de\r\n",
      "    2000    45448 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.bpe.en\r\n",
      "    2000    39224 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\r\n",
      "    2000    44390 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\r\n",
      "    2000    42299 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.bpe.de\r\n",
      "    2000    46237 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.bpe.en\r\n",
      "    2000    39841 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\r\n",
      "    2000    45237 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en\r\n",
      "  360000  7827724 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -lw /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the encoded files used for training\n",
    "#os.environ[\"dataset\"]=\"./de-en-baseline/data\"\n",
    "os.environ[\"dataset\"]= \"/home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|####################################| 32000/32000 [00:55<00:00, 581.48it/s]\r\n",
      "bpe.codes.32000  dev.de       test.bpe.en  train.bpe.de  train.en\r\n",
      "dev.bpe.de\t dev.en       test.de\t   train.bpe.en  vocab.txt\r\n",
      "dev.bpe.en\t test.bpe.de  test.en\t   train.de\r\n",
      "bpe.codes.32000  dev.bpe.en  test.bpe.de  test.en\ttrain.de\r\n",
      "data\t\t dev.de      test.bpe.en  train.bpe.de\ttrain.en\r\n",
      "dev.bpe.de\t dev.en      test.de\t  train.bpe.en\r\n",
      "BPE Test language Sentences\r\n",
      "the member states indicated in the annex are authorised to allow secondary crops to be cultivated on the eligible hectares during a period of maximum three months starting each year on the date laid down in that annex for each member state\r\n",
      "an annex as set out in the annex to this regulation is added\r\n",
      "this regulation shall enter into force on the day of its publication in the official journal of the european union\r\n",
      "it shall apply from january\r\n",
      "this regulation shall be binding in its entirety and directly applicable in all member states\r\n",
      "Combined BPE Vocab\r\n",
      "entfal@@\r\n",
      "pub@@\r\n",
      "vorsitz@@\r\n",
      "südost@@\r\n",
      "samem\r\n",
      "sieren\r\n",
      "blum@@\r\n",
      "stick@@\r\n",
      "verit@@\r\n",
      "zerr@@\r\n"
     ]
    }
   ],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input $dataset/train.$src $dataset/train.$tgt -s 32000 \\\n",
    "                                -o $dataset/bpe.codes.32000 --write-vocabulary $dataset/vocab.$src $dataset/vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src --dropout 0.1 --seed 42 < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt --dropout 0.1 --seed 42 < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/dev.$src > $dataset/dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/dev.$tgt > $dataset/dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/test.$src > $dataset/test.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/test.$tgt > $dataset/test.bpe.$tgt\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp $dataset/train.* \"$data_path\"\n",
    "! cp $dataset/test.* \"$data_path\"\n",
    "! cp $dataset/dev.* \"$data_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp $dataset/train.* \"$env_path\"\n",
    "! cp $dataset/test.* \"$env_path\"\n",
    "! cp $dataset/dev.* \"$env_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$env_path\"\n",
    "! ls \"$env_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! chmod +x joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py $data_path/train.bpe.$src $data_path/train.bpe.$tgt --output_path $data_path/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE Test language Sentences\"\n",
    "! tail -n 5 $dataset/test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "env_path = os.environ[\"env_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data/{name}/vocab.txt\"\n",
    "    trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # sacrebleu options\n",
    "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
    "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
    "\n",
    "training:\n",
    "    #load_model: \"{env_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    #load_model: \"models/deen_transformer/455000.ckpt\"\n",
    "    load_model: \"models/pre_model/455000.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"noam\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.0          #set to 0.1 to turn on\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"loss\"   # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    epochs: 45                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"            # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    #model_dir: \"models/{name}_transformer\"\n",
    "    model_dir: \"models/pre_model\"\n",
    "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 60\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "\"\"\".format(name=name, env_path=os.environ[\"env_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-13 00:15:21,232 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\r\n",
      "2022-07-13 00:15:21,469 - INFO - joeynmt.data - Loading training data...\r\n",
      "2022-07-13 00:15:23,456 - INFO - joeynmt.data - Building vocabulary...\r\n",
      "2022-07-13 00:15:38,065 - INFO - joeynmt.data - Loading dev data...\r\n",
      "2022-07-13 00:15:38,116 - INFO - joeynmt.data - Loading test data...\r\n",
      "2022-07-13 00:15:38,146 - INFO - joeynmt.data - Data loaded.\r\n",
      "2022-07-13 00:15:38,146 - INFO - joeynmt.model - Building an encoder-decoder model...\r\n",
      "2022-07-13 00:15:39,638 - INFO - joeynmt.model - Enc-dec model built.\r\n",
      "2022-07-13 00:15:53.578434: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/software/IPython/7.9.0-fosscuda-2019b-Python-3.7.4/lib:/software/software/libyaml/0.2.2-GCCcore-8.3.0/lib:/software/software/matplotlib/3.1.1-fosscuda-2019b-Python-3.7.4/lib:/software/software/Tkinter/3.7.4-GCCcore-8.3.0/lib:/software/software/Tk/8.6.9-GCCcore-8.3.0/lib:/software/software/X11/20190717-GCCcore-8.3.0/lib:/software/software/fontconfig/2.13.1-GCCcore-8.3.0/lib:/software/software/expat/2.2.7-GCCcore-8.3.0/lib:/software/software/freetype/2.10.1-GCCcore-8.3.0/lib:/software/software/libpng/1.6.37-GCCcore-8.3.0/lib:/software/software/ZeroMQ/4.3.2-GCCcore-8.3.0/lib:/software/software/util-linux/2.34-GCCcore-8.3.0/lib:/software/software/libsodium/1.0.18-GCCcore-8.3.0/lib:/software/software/OpenPGM/5.2.122-GCCcore-8.3.0/lib:/software/software/Python/3.7.4-GCCcore-8.3.0/lib:/software/software/libffi/3.2.1-GCCcore-8.3.0/lib64:/software/software/libffi/3.2.1-GCCcore-8.3.0/lib:/software/software/GMP/6.1.2-GCCcore-8.3.0/lib:/software/software/SQLite/3.29.0-GCCcore-8.3.0/lib:/software/software/Tcl/8.6.9-GCCcore-8.3.0/lib:/software/software/libreadline/8.0-GCCcore-8.3.0/lib:/software/software/ncurses/6.1-GCCcore-8.3.0/lib:/software/software/bzip2/1.0.8-GCCcore-8.3.0/lib:/software/software/ScaLAPACK/2.0.2-gompic-2019b/lib:/software/software/FFTW/3.3.8-gompic-2019b/lib:/software/software/OpenBLAS/0.3.7-GCC-8.3.0/lib:/software/software/OpenMPI/3.1.4-gcccuda-2019b/lib:/software/software/hwloc/1.11.12-GCCcore-8.3.0/lib:/software/software/libpciaccess/0.14-GCCcore-8.3.0/lib:/software/software/libxml2/2.9.9-GCCcore-8.3.0/lib:/software/software/XZ/5.2.4-GCCcore-8.3.0/lib:/software/software/numactl/2.0.12-GCCcore-8.3.0/lib:/software/software/CUDA/10.1.243-GCC-8.3.0/nvvm/lib64:/software/software/CUDA/10.1.243-GCC-8.3.0/extras/CUPTI/lib64:/software/software/CUDA/10.1.243-GCC-8.3.0/lib64:/software/software/binutils/2.32-GCCcore-8.3.0/lib:/software/software/zlib/1.2.11-GCCcore-8.3.0/lib:/software/software/GCCcore/8.3.0/lib/gcc/x86_64-pc-linux-gnu/8.3.0:/software/software/GCCcore/8.3.0/lib64:/software/software/GCCcore/8.3.0/lib\r\n",
      "2022-07-13 00:15:53.578531: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2022-07-13 00:17:10,867 - INFO - joeynmt.training - Total params: 59593728\r\n",
      "2022-07-13 00:17:21,387 - INFO - joeynmt.training - Loading model from models/pre_model/455000.ckpt\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 48, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 35, in main\r\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/training.py\", line 825, in train\r\n",
      "    trainer = TrainManager(model=model, config=cfg)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/training.py\", line 220, in __init__\r\n",
      "    reset_iter_state=train_config.get(\"reset_iter_state\", False))\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/training.py\", line 332, in init_from_checkpoint\r\n",
      "    self.model.load_state_dict(model_checkpoint[\"model_state\"])\r\n",
      "  File \"/data/s3886026/envs/jnmt_env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1407, in load_state_dict\r\n",
      "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\r\n",
      "RuntimeError: Error(s) in loading state_dict for Model:\r\n",
      "\tsize mismatch for src_embed.lut.weight: copying a param with shape torch.Size([31187, 512]) from checkpoint, the shape in current model is torch.Size([30182, 512]).\r\n",
      "\tsize mismatch for trg_embed.lut.weight: copying a param with shape torch.Size([31187, 512]) from checkpoint, the shape in current model is torch.Size([30182, 512]).\r\n",
      "\tsize mismatch for decoder.output_layer.weight: copying a param with shape torch.Size([31187, 512]) from checkpoint, the shape in current model is torch.Size([30182, 512]).\r\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 48, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 38, in main\r\n",
      "    output_path=args.output_path, save_attention=args.save_attention)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/prediction.py\", line 279, in test\r\n",
      "    cfg = load_config(cfg_file)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/helpers.py\", line 180, in load_config\r\n",
      "    with open(path, 'r', encoding=\"utf-8\") as ymlfile:\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'models/deen_transformer/config.yaml'\r\n"
     ]
    }
   ],
   "source": [
    "!cd joeynmt; python3 -m joeynmt test models/deen_transformer/config.yaml --output_path models/deen_transformer/predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jnmt_env",
   "language": "python",
   "name": "jnmt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
