{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/s3886026/NLP_thesis/JoeyNMT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
    "# These will also become the suffix's of all vocab and corpus files used throughout\n",
    "source_language = \"de\"\n",
    "target_language = \"en\" \n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
    "\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"./$src-$tgt-$tag\"\n",
    "os.environ[\"env_path\"] = \"./%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the file paths here\n",
    "# source_file = \"./corpus/EMEA/MOSES/EMEA.clean.de\"\n",
    "# target_file = \"./corpus/EMEA/MOSES/EMEA.clean.en\"\n",
    "source_file = \"./out-domain/OpenSubtitle/op_st2.clean.de\"\n",
    "target_file = \"./out-domain/OpenSubtitle/op_st2.clean.en\"\n",
    "\n",
    "# They should both have the same length.\n",
    "! wc -l \"$source_file\"\n",
    "! wc -l \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 15 \"$source_file\" \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is need for tokenization, thise can be done via sacremoses\n",
    "tok_source_file = source_file+\".tok\"\n",
    "tok_target_file = target_file+\".tok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the source\n",
    "!sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
    "# Tokenize the target\n",
    "!sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look what tokenization did to the text.\n",
    "! head \"$source_file\"*\n",
    "! head \"$target_file\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the pointers to our files such that we continue to work with the tokenized data.\n",
    "source_file = tok_source_file\n",
    "target_file = tok_target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p /de-en-baseline/data\n",
    "#\"/home/s3886026/NLP_thesis/JoeyNMT/translate.ipynb\"\n",
    "#os.environ[\"DIR\"] = \"/home/s3886026/NLP_thesis/JoeyNMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 900000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 900000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 7000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 7000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 910000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 910000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 8000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 8000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "!head -n 96000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "!head -n 96000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "!tail -n 4000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "!tail -n 4000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "!tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "!tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 776126 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 776126 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 332625 \"$source_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 332625 \"$target_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 110875 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 110875 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -lw /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the encoded files used for training\n",
    "#os.environ[\"dataset\"]=\"./de-en-baseline/data\"\n",
    "os.environ[\"dataset\"]= \"/home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input $dataset/train.$src $dataset/train.$tgt -s 32000 \\\n",
    "                                -o $dataset/bpe.codes.32000 --write-vocabulary $dataset/vocab.$src $dataset/vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src --dropout 0.1 --seed 42 < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt --dropout 0.1 --seed 42 < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/dev.$src > $dataset/dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/dev.$tgt > $dataset/dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/test.$src > $dataset/test.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/test.$tgt > $dataset/test.bpe.$tgt\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp $dataset/train.* \"$data_path\"\n",
    "! cp $dataset/test.* \"$data_path\"\n",
    "! cp $dataset/dev.* \"$data_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp $dataset/train.* \"$env_path\"\n",
    "! cp $dataset/test.* \"$env_path\"\n",
    "! cp $dataset/dev.* \"$env_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$env_path\"\n",
    "! ls \"$env_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! chmod +x joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py $data_path/train.bpe.$src $data_path/train.bpe.$tgt --output_path $data_path/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE Test language Sentences\"\n",
    "! tail -n 5 $dataset/test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "env_path = os.environ[\"env_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data3/vocab.txt\"\n",
    "    trg_vocab: \"data3/vocab.txt\"\n",
    "    #src_vocab: \"data/{name}/vocab.txt\"\n",
    "    #trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # sacrebleu options\n",
    "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
    "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
    "\n",
    "training:\n",
    "    #load_model: \"{env_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    #load_model: \"models/deen_transformer/455000.ckpt\"\n",
    "    load_model: \"models/pre_model/455000.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"noam\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.0          #set to 0.1 to turn on\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"loss\"   # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    epochs: 45                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"            # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    #model_dir: \"models/{name}_transformer\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 60\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "\"\"\".format(name=name, env_path=os.environ[\"env_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/software/software/Python/3.7.4-GCCcore-8.3.0/bin/python3: No module named joeynmt.__main__; 'joeynmt' is a package and cannot be directly executed\r\n"
     ]
    }
   ],
   "source": [
    "# !cd joeynmt; python3 -m joeynmt test models/deen_transformer/config.yaml --output_path models/deen_transformer/predictions\n",
    "!cd joeynmt; python3 -m joeynmt test models/deen_transformer/config.yaml --output_path models/deen_transformer/predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jnmt_env",
   "language": "python",
   "name": "jnmt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
