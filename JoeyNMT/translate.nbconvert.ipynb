{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/s3886026/NLP_thesis/JoeyNMT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
    "# These will also become the suffix's of all vocab and corpus files used throughout\n",
    "source_language = \"de\"\n",
    "target_language = \"en\" \n",
    "lc = False  # If True, lowercase the data.\n",
    "seed = 42  # Random seed for shuffling.\n",
    "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
    "\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "os.environ[\"tag\"] = tag\n",
    "\n",
    "# This will save it to a folder in our gdrive instead!\n",
    "!mkdir -p \"./$src-$tgt-$tag\"\n",
    "os.environ[\"env_path\"] = \"./%s-%s-%s\" % (source_language, target_language, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999 ./out-domain/OpenSubtitle/op_st2.clean.de\r\n",
      "99999 ./out-domain/OpenSubtitle/op_st2.clean.en\r\n"
     ]
    }
   ],
   "source": [
    "# specify the file paths here\n",
    "# source_file = \"./corpus/EMEA/MOSES/EMEA.clean.de\"\n",
    "# target_file = \"./corpus/EMEA/MOSES/EMEA.clean.en\"\n",
    "source_file = \"./out-domain/OpenSubtitle/op_st2.clean.de\"\n",
    "target_file = \"./out-domain/OpenSubtitle/op_st2.clean.en\"\n",
    "\n",
    "# They should both have the same length.\n",
    "! wc -l \"$source_file\"\n",
    "! wc -l \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./out-domain/OpenSubtitle/op_st2.clean.de <==\r\n",
      "Jetzt sei kein Arsch\r\n",
      "Wie hast du Brian kennengelernt\r\n",
      "Das weiß ich nicht mehr\r\n",
      "Ehrlich nicht\r\n",
      "Bei Vincent weiß ich es noch genau\r\n",
      "Es war neun Monate vor Nats Geburt\r\n",
      "Im Ernst\r\n",
      "Ich glaube das nennt man einen Volltreffer\r\n",
      "Wie schaffst du es nur Kinder zu erziehen und zu unterrichten\r\n",
      "Ich bin keine Lehrerin\r\n",
      "Tut mir leid\r\n",
      "Ich dachte Brian hätte gesagt du wärst die Lehrerin deines Sohnes\r\n",
      "Er hat wohl einen anderen Vincent gemeint\r\n",
      "Ich glaube ich hätte jetzt doch gern was zu trinken\r\n",
      "Entschuldigt mich\r\n",
      "\r\n",
      "==> ./out-domain/OpenSubtitle/op_st2.clean.en <==\r\n",
      "Dont be a dick\r\n",
      "So Maureen How did you and Brian meet\r\n",
      "I cant remember\r\n",
      "Really\r\n",
      "Well I remember exactly the day I met Vincent\r\n",
      "It was nine months before Nat was born\r\n",
      "Really it was\r\n",
      "I think you call that a hole in one\r\n",
      "I dont know how you find the time to raise kids and teach\r\n",
      "Oh Im not a teacher\r\n",
      "Oh sorry\r\n",
      "I couldve sworn Brian said you were your sons teacher My mistake\r\n",
      "Mustve been talking about another Vincent\r\n",
      "Actually you know what I think I do fancy that drink after all\r\n",
      "Excuse me\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 15 \"$source_file\" \"$target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (0.0.49)\r\n",
      "Requirement already satisfied: click in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (7.1.2)\r\n",
      "Requirement already satisfied: tqdm in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (4.64.0)\r\n",
      "Requirement already satisfied: joblib in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (1.1.0)\r\n",
      "Requirement already satisfied: six in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (1.16.0)\r\n",
      "Requirement already satisfied: regex in /data/s3886026/envs/jnmt_env/lib/python3.7/site-packages (from sacremoses) (2022.3.15)\r\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.1 is available.\r\n",
      "You should consider upgrading via the '/data/s3886026/envs/jnmt_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is need for tokenization, thise can be done via sacremoses\n",
    "tok_source_file = source_file+\".tok\"\n",
    "tok_target_file = target_file+\".tok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the source\n",
    "!sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
    "# Tokenize the target\n",
    "!sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./out-domain/OpenSubtitle/op_st2.clean.de <==\r\n",
      "Jetzt sei kein Arsch\r\n",
      "Wie hast du Brian kennengelernt\r\n",
      "Das weiß ich nicht mehr\r\n",
      "Ehrlich nicht\r\n",
      "Bei Vincent weiß ich es noch genau\r\n",
      "Es war neun Monate vor Nats Geburt\r\n",
      "Im Ernst\r\n",
      "Ich glaube das nennt man einen Volltreffer\r\n",
      "Wie schaffst du es nur Kinder zu erziehen und zu unterrichten\r\n",
      "Ich bin keine Lehrerin\r\n",
      "\r\n",
      "==> ./out-domain/OpenSubtitle/op_st2.clean.de.tok <==\r\n",
      "Jetzt sei kein Arsch\r\n",
      "Wie hast du Brian kennengelernt\r\n",
      "Das weiß ich nicht mehr\r\n",
      "Ehrlich nicht\r\n",
      "Bei Vincent weiß ich es noch genau\r\n",
      "Es war neun Monate vor Nats Geburt\r\n",
      "Im Ernst\r\n",
      "Ich glaube das nennt man einen Volltreffer\r\n",
      "Wie schaffst du es nur Kinder zu erziehen und zu unterrichten\r\n",
      "Ich bin keine Lehrerin\r\n",
      "==> ./out-domain/OpenSubtitle/op_st2.clean.en <==\r\n",
      "Dont be a dick\r\n",
      "So Maureen How did you and Brian meet\r\n",
      "I cant remember\r\n",
      "Really\r\n",
      "Well I remember exactly the day I met Vincent\r\n",
      "It was nine months before Nat was born\r\n",
      "Really it was\r\n",
      "I think you call that a hole in one\r\n",
      "I dont know how you find the time to raise kids and teach\r\n",
      "Oh Im not a teacher\r\n",
      "\r\n",
      "==> ./out-domain/OpenSubtitle/op_st2.clean.en.tok <==\r\n",
      "Dont be a dick\r\n",
      "So Maureen How did you and Brian meet\r\n",
      "I cant remember\r\n",
      "Really\r\n",
      "Well I remember exactly the day I met Vincent\r\n",
      "It was nine months before Nat was born\r\n",
      "Really it was\r\n",
      "I think you call that a hole in one\r\n",
      "I dont know how you find the time to raise kids and teach\r\n",
      "Oh Im not a teacher\r\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look what tokenization did to the text.\n",
    "! head \"$source_file\"*\n",
    "! head \"$target_file\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the pointers to our files such that we continue to work with the tokenized data.\n",
    "source_file = tok_source_file\n",
    "target_file = tok_target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p /de-en-baseline/data\n",
    "#\"/home/s3886026/NLP_thesis/JoeyNMT/translate.ipynb\"\n",
    "#os.environ[\"DIR\"] = \"/home/s3886026/NLP_thesis/JoeyNMT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 900000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 900000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 7000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 7000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 910000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 910000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 8000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 8000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail: write error\r\n",
      "tail: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "!head -n 96000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "!head -n 96000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "!tail -n 4000 \"$source_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "!tail -n 4000 \"$target_file\" | head -n 2000 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "!tail -n 2000 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "!tail -n 2000 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset by 70%,20%,10% for train, dev, test\n",
    "# !head -n 776126 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\n",
    "# !head -n 776126 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\n",
    "# !tail -n 332625 \"$source_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\n",
    "# !tail -n 332625 \"$target_file\" | head -n 221750 > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\n",
    "# !tail -n 110875 \"$source_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\n",
    "# !tail -n 110875 \"$target_file\" > /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   86000  1403453 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.bpe.de\r\n",
      "   86000  1480242 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.bpe.en\r\n",
      "   96000   637576 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.de\r\n",
      "   96000   720827 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.en\r\n",
      "    2000    35729 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.bpe.de\r\n",
      "    2000    40553 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.bpe.en\r\n",
      "    2000    12772 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.de\r\n",
      "    2000    13736 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.en\r\n",
      "    2000    26012 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.bpe.de\r\n",
      "    2000    30735 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.bpe.en\r\n",
      "    2000    12395 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.de\r\n",
      "    2000    15609 /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.en\r\n",
      "  380000  4429639 total\r\n"
     ]
    }
   ],
   "source": [
    "!wc -lw /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/train.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/dev.* \\\n",
    "        /home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data/test.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the encoded files used for training\n",
    "#os.environ[\"dataset\"]=\"./de-en-baseline/data\"\n",
    "os.environ[\"dataset\"]= \"/home/s3886026/NLP_thesis/JoeyNMT/de-en-baseline/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|####################################| 32000/32000 [01:04<00:00, 496.34it/s]\r\n",
      "bpe.codes.32000  dev.de       test.bpe.en  train.bpe.de  train.en\r\n",
      "dev.bpe.de\t dev.en       test.de\t   train.bpe.en  vocab.txt\r\n",
      "dev.bpe.en\t test.bpe.de  test.en\t   train.de\r\n",
      "bpe.codes.32000  dev.bpe.en  test.bpe.de  test.en\ttrain.de\r\n",
      "data\t\t dev.de      test.bpe.en  train.bpe.de\ttrain.en\r\n",
      "dev.bpe.de\t dev.en      test.de\t  train.bpe.en\r\n",
      "BPE Test language Sentences\r\n",
      "Cause I got something for Mothers Day also\r\n",
      "Ri@@ ley Im gonna cry\r\n",
      "Dont\r\n",
      "Dont cry Well I will\r\n",
      "Well dont\r\n",
      "Combined BPE Vocab\r\n",
      "fahrungen\r\n",
      "redi@@\r\n",
      "verhe@@\r\n",
      "brechens\r\n",
      "tsetzen\r\n",
      "ichtungs@@\r\n",
      "andau@@\r\n",
      "turm\r\n",
      "ropä@@\r\n",
      "aga@@\r\n"
     ]
    }
   ],
   "source": [
    "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
    "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
    "\n",
    "# Do subword NMT\n",
    "from os import path\n",
    "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
    "os.environ[\"tgt\"] = target_language\n",
    "\n",
    "# Learn BPEs on the training data.\n",
    "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
    "! subword-nmt learn-joint-bpe-and-vocab --input $dataset/train.$src $dataset/train.$tgt -s 32000 \\\n",
    "                                -o $dataset/bpe.codes.32000 --write-vocabulary $dataset/vocab.$src $dataset/vocab.$tgt\n",
    "\n",
    "# Apply BPE splits to the development and test data.\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src --dropout 0.1 --seed 42 < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "# ! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt --dropout 0.1 --seed 42 < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/train.$src > $dataset/train.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/train.$tgt > $dataset/train.bpe.$tgt\n",
    "\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/dev.$src > $dataset/dev.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/dev.$tgt > $dataset/dev.bpe.$tgt\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$src < $dataset/test.$src > $dataset/test.bpe.$src\n",
    "! subword-nmt apply-bpe -c $dataset/bpe.codes.32000 --vocabulary $dataset/vocab.$tgt < $dataset/test.$tgt > $dataset/test.bpe.$tgt\n",
    "\n",
    "# Create directory, move everyone we care about to the correct location\n",
    "! mkdir -p \"$data_path\"\n",
    "! cp $dataset/train.* \"$data_path\"\n",
    "! cp $dataset/test.* \"$data_path\"\n",
    "! cp $dataset/dev.* \"$data_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$data_path\"\n",
    "! ls \"$data_path\"\n",
    "\n",
    "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
    "! cp $dataset/train.* \"$env_path\"\n",
    "! cp $dataset/test.* \"$env_path\"\n",
    "! cp $dataset/dev.* \"$env_path\"\n",
    "! cp $dataset/bpe.codes.32000 \"$env_path\"\n",
    "! ls \"$env_path\"\n",
    "\n",
    "# Create that vocab using build_vocab\n",
    "! chmod +x joeynmt/scripts/build_vocab.py\n",
    "! joeynmt/scripts/build_vocab.py $data_path/train.bpe.$src $data_path/train.bpe.$tgt --output_path $data_path/vocab.txt\n",
    "\n",
    "# Some output\n",
    "! echo \"BPE Test language Sentences\"\n",
    "! tail -n 5 $dataset/test.bpe.$tgt\n",
    "! echo \"Combined BPE Vocab\"\n",
    "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
    "# (You can of course play with all the parameters if you'd like!)\n",
    "\n",
    "name = '%s%s' % (source_language, target_language)\n",
    "env_path = os.environ[\"env_path\"]\n",
    "\n",
    "# Create the config\n",
    "config = \"\"\"\n",
    "name: \"{name}_transformer\"\n",
    "\n",
    "data:\n",
    "    src: \"{source_language}\"\n",
    "    trg: \"{target_language}\"\n",
    "    train: \"data/{name}/train.bpe\"\n",
    "    dev:   \"data/{name}/dev.bpe\"\n",
    "    test:  \"data/{name}/test.bpe\"\n",
    "    level: \"bpe\"\n",
    "    lowercase: False\n",
    "    max_sent_length: 100\n",
    "    src_vocab: \"data3/vocab.txt\"\n",
    "    trg_vocab: \"data3/vocab.txt\"\n",
    "    #src_vocab: \"data/{name}/vocab.txt\"\n",
    "    #trg_vocab: \"data/{name}/vocab.txt\"\n",
    "\n",
    "testing:\n",
    "    beam_size: 5\n",
    "    alpha: 1.0\n",
    "    sacrebleu:                      # sacrebleu options\n",
    "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
    "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
    "\n",
    "training:\n",
    "    #load_model: \"{env_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
    "    #load_model: \"models/deen_transformer/455000.ckpt\"\n",
    "    load_model: \"models/pre_model/455000.ckpt\"\n",
    "    random_seed: 42\n",
    "    optimizer: \"adam\"\n",
    "    normalization: \"tokens\"\n",
    "    adam_betas: [0.9, 0.999] \n",
    "    scheduling: \"noam\"           # TODO: try switching from plateau to Noam scheduling\n",
    "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
    "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
    "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
    "    decrease_factor: 0.7\n",
    "    loss: \"crossentropy\"\n",
    "    learning_rate: 0.0003\n",
    "    learning_rate_min: 0.00000001\n",
    "    weight_decay: 0.0\n",
    "    label_smoothing: 0.0          #set to 0.1 to turn on\n",
    "    batch_size: 4096\n",
    "    batch_type: \"token\"\n",
    "    eval_batch_size: 3600\n",
    "    eval_batch_type: \"token\"\n",
    "    batch_multiplier: 1\n",
    "    early_stopping_metric: \"loss\"   # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    epochs: 45                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
    "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
    "    logging_freq: 100\n",
    "    eval_metric: \"bleu\"            # perplexity=>\"ppl\" or bleu score => \"bleu\"\n",
    "    #model_dir: \"models/{name}_transformer\"\n",
    "    model_dir: \"models/{name}_transformer\"\n",
    "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
    "    shuffle: True\n",
    "    use_cuda: True\n",
    "    max_output_length: 60\n",
    "    print_valid_sents: [0, 1, 2, 3]\n",
    "    keep_best_ckpts: 3\n",
    "\n",
    "model:\n",
    "    initializer: \"xavier\"\n",
    "    bias_initializer: \"zeros\"\n",
    "    init_gain: 1.0\n",
    "    embed_initializer: \"xavier\"\n",
    "    embed_init_gain: 1.0\n",
    "    tied_embeddings: True\n",
    "    tied_softmax: True\n",
    "    encoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8             # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512   # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "    decoder:\n",
    "        type: \"transformer\"\n",
    "        num_layers: 6\n",
    "        num_heads: 8              # TODO: Increase to 8 for larger data.\n",
    "        embeddings:\n",
    "            embedding_dim: 512    # TODO: Increase to 512 for larger data.\n",
    "            scale: True\n",
    "            dropout: 0.2\n",
    "        # typically ff_size = 4 x hidden_size\n",
    "        hidden_size: 512         # TODO: Increase to 512 for larger data.\n",
    "        ff_size: 2048            # TODO: Increase to 2048 for larger data.\n",
    "        dropout: 0.2\n",
    "\"\"\".format(name=name, env_path=os.environ[\"env_path\"], source_language=source_language, target_language=target_language)\n",
    "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 18:55:52,288 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\r\n",
      "2022-08-02 18:55:52,437 - INFO - joeynmt.data - Loading training data...\r\n",
      "2022-08-02 18:55:53,470 - INFO - joeynmt.data - Building vocabulary...\r\n",
      "2022-08-02 18:56:08,441 - INFO - joeynmt.data - Loading dev data...\r\n",
      "2022-08-02 18:56:08,675 - INFO - joeynmt.data - Loading test data...\r\n",
      "2022-08-02 18:56:08,691 - INFO - joeynmt.data - Data loaded.\r\n",
      "2022-08-02 18:56:08,691 - INFO - joeynmt.model - Building an encoder-decoder model...\r\n",
      "2022-08-02 18:56:10,238 - INFO - joeynmt.model - Enc-dec model built.\r\n",
      "2022-08-02 18:56:24.058725: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/software/IPython/7.9.0-fosscuda-2019b-Python-3.7.4/lib:/software/software/libyaml/0.2.2-GCCcore-8.3.0/lib:/software/software/matplotlib/3.1.1-fosscuda-2019b-Python-3.7.4/lib:/software/software/Tkinter/3.7.4-GCCcore-8.3.0/lib:/software/software/Tk/8.6.9-GCCcore-8.3.0/lib:/software/software/X11/20190717-GCCcore-8.3.0/lib:/software/software/fontconfig/2.13.1-GCCcore-8.3.0/lib:/software/software/expat/2.2.7-GCCcore-8.3.0/lib:/software/software/freetype/2.10.1-GCCcore-8.3.0/lib:/software/software/libpng/1.6.37-GCCcore-8.3.0/lib:/software/software/ZeroMQ/4.3.2-GCCcore-8.3.0/lib:/software/software/util-linux/2.34-GCCcore-8.3.0/lib:/software/software/libsodium/1.0.18-GCCcore-8.3.0/lib:/software/software/OpenPGM/5.2.122-GCCcore-8.3.0/lib:/software/software/Python/3.7.4-GCCcore-8.3.0/lib:/software/software/libffi/3.2.1-GCCcore-8.3.0/lib64:/software/software/libffi/3.2.1-GCCcore-8.3.0/lib:/software/software/GMP/6.1.2-GCCcore-8.3.0/lib:/software/software/SQLite/3.29.0-GCCcore-8.3.0/lib:/software/software/Tcl/8.6.9-GCCcore-8.3.0/lib:/software/software/libreadline/8.0-GCCcore-8.3.0/lib:/software/software/ncurses/6.1-GCCcore-8.3.0/lib:/software/software/bzip2/1.0.8-GCCcore-8.3.0/lib:/software/software/ScaLAPACK/2.0.2-gompic-2019b/lib:/software/software/FFTW/3.3.8-gompic-2019b/lib:/software/software/OpenBLAS/0.3.7-GCC-8.3.0/lib:/software/software/OpenMPI/3.1.4-gcccuda-2019b/lib:/software/software/hwloc/1.11.12-GCCcore-8.3.0/lib:/software/software/libpciaccess/0.14-GCCcore-8.3.0/lib:/software/software/libxml2/2.9.9-GCCcore-8.3.0/lib:/software/software/XZ/5.2.4-GCCcore-8.3.0/lib:/software/software/numactl/2.0.12-GCCcore-8.3.0/lib:/software/software/CUDA/10.1.243-GCC-8.3.0/nvvm/lib64:/software/software/CUDA/10.1.243-GCC-8.3.0/extras/CUPTI/lib64:/software/software/CUDA/10.1.243-GCC-8.3.0/lib64:/software/software/binutils/2.32-GCCcore-8.3.0/lib:/software/software/zlib/1.2.11-GCCcore-8.3.0/lib:/software/software/GCCcore/8.3.0/lib/gcc/x86_64-pc-linux-gnu/8.3.0:/software/software/GCCcore/8.3.0/lib64:/software/software/GCCcore/8.3.0/lib\r\n",
      "2022-08-02 18:56:24.058797: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n",
      "2022-08-02 18:58:07,561 - INFO - joeynmt.training - Total params: 60108288\r\n",
      "2022-08-02 18:58:17,909 - INFO - joeynmt.training - Loading model from models/pre_model/455000.ckpt\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                           cfg.name : deen_transformer\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                       cfg.data.src : de\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                       cfg.data.trg : en\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                     cfg.data.train : data/deen/train.bpe\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                       cfg.data.dev : data/deen/dev.bpe\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                      cfg.data.test : data/deen/test.bpe\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                     cfg.data.level : bpe\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                 cfg.data.lowercase : False\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -           cfg.data.max_sent_length : 100\r\n",
      "2022-08-02 18:58:48,173 - INFO - joeynmt.helpers -                 cfg.data.src_vocab : data3/vocab.txt\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -                 cfg.data.trg_vocab : data3/vocab.txt\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -     cfg.testing.sacrebleu.tokenize : none\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -            cfg.training.load_model : models/pre_model/455000.ckpt\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -            cfg.training.adam_betas : [0.9, 0.999]\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -            cfg.training.scheduling : noam\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -              cfg.training.patience : 5\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -  cfg.training.learning_rate_factor : 0.5\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -  cfg.training.learning_rate_warmup : 1000\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7\r\n",
      "2022-08-02 18:58:48,174 - INFO - joeynmt.helpers -                  cfg.training.loss : crossentropy\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -     cfg.training.learning_rate_min : 1e-08\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.0\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -            cfg.training.batch_size : 4096\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -            cfg.training.batch_type : token\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 3600\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -      cfg.training.batch_multiplier : 1\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : loss\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -                cfg.training.epochs : 45\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 1000\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/deen_transformer\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -             cfg.training.overwrite : True\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -               cfg.training.shuffle : True\r\n",
      "2022-08-02 18:58:48,175 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 60\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3]\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -       cfg.training.keep_best_ckpts : 3\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 6\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 8\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 512\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 2048\r\n",
      "2022-08-02 18:58:48,176 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.2\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 6\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 8\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 512\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 2048\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.2\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - Data set sizes: \r\n",
      "\ttrain 95995,\r\n",
      "\tvalid 1999,\r\n",
      "\ttest 2000\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - First training example:\r\n",
      "\t[SRC] Jetzt sei kein Arsch\r\n",
      "\t[TRG] Dont be a dick\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) of (6) in (7) der (8) die (9) and\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) of (6) in (7) der (8) die (9) and\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - Number of Src words (types): 31187\r\n",
      "2022-08-02 18:58:48,177 - INFO - joeynmt.helpers - Number of Trg words (types): 31187\r\n",
      "2022-08-02 18:58:48,178 - INFO - joeynmt.training - Model(\r\n",
      "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\r\n",
      "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\r\n",
      "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=31187),\r\n",
      "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=31187))\r\n",
      "2022-08-02 18:58:48,204 - INFO - joeynmt.training - Train stats:\r\n",
      "\tdevice: cuda\r\n",
      "\tn_gpu: 1\r\n",
      "\t16-bits training: False\r\n",
      "\tgradient accumulation: 1\r\n",
      "\tbatch size per device: 4096\r\n",
      "\ttotal batch size (w. parallel & accumulation): 4096\r\n",
      "2022-08-02 18:58:48,204 - INFO - joeynmt.training - EPOCH 1\r\n",
      "2022-08-02 18:58:48,516 - INFO - joeynmt.training - Epoch   1: total training loss 0.00\r\n",
      "2022-08-02 18:58:48,516 - INFO - joeynmt.training - EPOCH 2\r\n",
      "2022-08-02 18:59:07,174 - INFO - joeynmt.training - Epoch   2, Step:   455100, Batch Loss:     4.066658, Tokens per Sec:     5216, Lr: 0.000033\r\n",
      "2022-08-02 18:59:25,604 - INFO - joeynmt.training - Epoch   2, Step:   455200, Batch Loss:     3.935544, Tokens per Sec:     5563, Lr: 0.000033\r\n",
      "2022-08-02 18:59:44,022 - INFO - joeynmt.training - Epoch   2, Step:   455300, Batch Loss:     3.618079, Tokens per Sec:     5416, Lr: 0.000033\r\n",
      "2022-08-02 19:00:02,477 - INFO - joeynmt.training - Epoch   2, Step:   455400, Batch Loss:     3.714134, Tokens per Sec:     5432, Lr: 0.000033\r\n",
      "2022-08-02 19:00:20,960 - INFO - joeynmt.training - Epoch   2, Step:   455500, Batch Loss:     3.621067, Tokens per Sec:     5433, Lr: 0.000033\r\n",
      "2022-08-02 19:00:39,480 - INFO - joeynmt.training - Epoch   2, Step:   455600, Batch Loss:     3.275266, Tokens per Sec:     5400, Lr: 0.000033\r\n",
      "2022-08-02 19:00:57,949 - INFO - joeynmt.training - Epoch   2, Step:   455700, Batch Loss:     3.416143, Tokens per Sec:     5480, Lr: 0.000033\r\n",
      "2022-08-02 19:01:16,408 - INFO - joeynmt.training - Epoch   2, Step:   455800, Batch Loss:     3.729363, Tokens per Sec:     5548, Lr: 0.000033\r\n",
      "2022-08-02 19:01:25,970 - INFO - joeynmt.training - Epoch   2: total training loss 3240.61\r\n",
      "2022-08-02 19:01:25,970 - INFO - joeynmt.training - EPOCH 3\r\n",
      "2022-08-02 19:01:34,656 - INFO - joeynmt.training - Epoch   3, Step:   455900, Batch Loss:     3.386894, Tokens per Sec:     5393, Lr: 0.000033\r\n",
      "2022-08-02 19:01:53,099 - INFO - joeynmt.training - Epoch   3, Step:   456000, Batch Loss:     3.208765, Tokens per Sec:     5513, Lr: 0.000033\r\n",
      "2022-08-02 19:02:23,600 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:02:23,601 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:02:23,602 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:02:23,602 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:02:23,602 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:02:23,602 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of a <unk> <unk> in the <unk> <unk> then <unk> to see how much <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:02:23,602 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step   456000: bleu:   0.47, loss: 53635.8828, ppl:  23.9097, duration: 30.5026s\r\n",
      "2022-08-02 19:02:42,128 - INFO - joeynmt.training - Epoch   3, Step:   456100, Batch Loss:     3.549402, Tokens per Sec:     5492, Lr: 0.000033\r\n",
      "2022-08-02 19:03:00,374 - INFO - joeynmt.training - Epoch   3, Step:   456200, Batch Loss:     3.195678, Tokens per Sec:     5621, Lr: 0.000033\r\n",
      "2022-08-02 19:03:18,794 - INFO - joeynmt.training - Epoch   3, Step:   456300, Batch Loss:     3.305254, Tokens per Sec:     5447, Lr: 0.000033\r\n",
      "2022-08-02 19:03:37,217 - INFO - joeynmt.training - Epoch   3, Step:   456400, Batch Loss:     3.428138, Tokens per Sec:     5344, Lr: 0.000033\r\n",
      "2022-08-02 19:03:55,867 - INFO - joeynmt.training - Epoch   3, Step:   456500, Batch Loss:     3.093119, Tokens per Sec:     5380, Lr: 0.000033\r\n",
      "2022-08-02 19:04:14,343 - INFO - joeynmt.training - Epoch   3, Step:   456600, Batch Loss:     3.315536, Tokens per Sec:     5376, Lr: 0.000033\r\n",
      "2022-08-02 19:04:32,771 - INFO - joeynmt.training - Epoch   3, Step:   456700, Batch Loss:     3.449538, Tokens per Sec:     5395, Lr: 0.000033\r\n",
      "2022-08-02 19:04:33,664 - INFO - joeynmt.training - Epoch   3: total training loss 2858.01\r\n",
      "2022-08-02 19:04:33,665 - INFO - joeynmt.training - EPOCH 4\r\n",
      "2022-08-02 19:04:51,218 - INFO - joeynmt.training - Epoch   4, Step:   456800, Batch Loss:     3.211407, Tokens per Sec:     5436, Lr: 0.000033\r\n",
      "2022-08-02 19:05:09,736 - INFO - joeynmt.training - Epoch   4, Step:   456900, Batch Loss:     3.430977, Tokens per Sec:     5438, Lr: 0.000033\r\n",
      "2022-08-02 19:05:28,167 - INFO - joeynmt.training - Epoch   4, Step:   457000, Batch Loss:     3.177852, Tokens per Sec:     5367, Lr: 0.000033\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk>\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:05:59,816 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> if a <unk> of a <unk> <unk> in the <unk> <unk> then <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:05:59,817 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step   457000: bleu:   0.73, loss: 51404.6445, ppl:  20.9520, duration: 31.6494s\r\n",
      "2022-08-02 19:06:18,107 - INFO - joeynmt.training - Epoch   4, Step:   457100, Batch Loss:     3.265148, Tokens per Sec:     5474, Lr: 0.000033\r\n",
      "2022-08-02 19:06:36,610 - INFO - joeynmt.training - Epoch   4, Step:   457200, Batch Loss:     3.221056, Tokens per Sec:     5398, Lr: 0.000033\r\n",
      "2022-08-02 19:06:55,120 - INFO - joeynmt.training - Epoch   4, Step:   457300, Batch Loss:     3.229146, Tokens per Sec:     5522, Lr: 0.000033\r\n",
      "2022-08-02 19:07:13,420 - INFO - joeynmt.training - Epoch   4, Step:   457400, Batch Loss:     3.212439, Tokens per Sec:     5422, Lr: 0.000033\r\n",
      "2022-08-02 19:07:31,798 - INFO - joeynmt.training - Epoch   4, Step:   457500, Batch Loss:     3.281465, Tokens per Sec:     5513, Lr: 0.000033\r\n",
      "2022-08-02 19:07:42,034 - INFO - joeynmt.training - Epoch   4: total training loss 2756.29\r\n",
      "2022-08-02 19:07:42,034 - INFO - joeynmt.training - EPOCH 5\r\n",
      "2022-08-02 19:07:50,475 - INFO - joeynmt.training - Epoch   5, Step:   457600, Batch Loss:     3.353206, Tokens per Sec:     5197, Lr: 0.000033\r\n",
      "2022-08-02 19:08:08,971 - INFO - joeynmt.training - Epoch   5, Step:   457700, Batch Loss:     3.225831, Tokens per Sec:     5404, Lr: 0.000033\r\n",
      "2022-08-02 19:08:27,652 - INFO - joeynmt.training - Epoch   5, Step:   457800, Batch Loss:     3.219278, Tokens per Sec:     5459, Lr: 0.000033\r\n",
      "2022-08-02 19:08:46,080 - INFO - joeynmt.training - Epoch   5, Step:   457900, Batch Loss:     3.230704, Tokens per Sec:     5509, Lr: 0.000033\r\n",
      "2022-08-02 19:09:04,624 - INFO - joeynmt.training - Epoch   5, Step:   458000, Batch Loss:     3.152630, Tokens per Sec:     5508, Lr: 0.000033\r\n",
      "2022-08-02 19:09:37,085 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:09:37,085 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> his <unk> but his <unk> <unk> the <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> two <unk> <unk> <unk>\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> it was so <unk> <unk> <unk>\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:09:37,086 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:09:37,087 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:09:37,087 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   458000: bleu:   1.08, loss: 50091.8203, ppl:  19.3858, duration: 32.4628s\r\n",
      "2022-08-02 19:09:55,411 - INFO - joeynmt.training - Epoch   5, Step:   458100, Batch Loss:     3.381351, Tokens per Sec:     5416, Lr: 0.000033\r\n",
      "2022-08-02 19:10:14,152 - INFO - joeynmt.training - Epoch   5, Step:   458200, Batch Loss:     3.328885, Tokens per Sec:     5568, Lr: 0.000033\r\n",
      "2022-08-02 19:10:32,652 - INFO - joeynmt.training - Epoch   5, Step:   458300, Batch Loss:     3.021114, Tokens per Sec:     5451, Lr: 0.000033\r\n",
      "2022-08-02 19:10:51,090 - INFO - joeynmt.training - Epoch   5, Step:   458400, Batch Loss:     3.165592, Tokens per Sec:     5400, Lr: 0.000033\r\n",
      "2022-08-02 19:10:51,433 - INFO - joeynmt.training - Epoch   5: total training loss 2681.48\r\n",
      "2022-08-02 19:10:51,434 - INFO - joeynmt.training - EPOCH 6\r\n",
      "2022-08-02 19:11:09,628 - INFO - joeynmt.training - Epoch   6, Step:   458500, Batch Loss:     3.062245, Tokens per Sec:     5400, Lr: 0.000033\r\n",
      "2022-08-02 19:11:28,165 - INFO - joeynmt.training - Epoch   6, Step:   458600, Batch Loss:     3.058940, Tokens per Sec:     5368, Lr: 0.000033\r\n",
      "2022-08-02 19:11:46,737 - INFO - joeynmt.training - Epoch   6, Step:   458700, Batch Loss:     3.091016, Tokens per Sec:     5349, Lr: 0.000033\r\n",
      "2022-08-02 19:12:05,338 - INFO - joeynmt.training - Epoch   6, Step:   458800, Batch Loss:     3.040527, Tokens per Sec:     5308, Lr: 0.000033\r\n",
      "2022-08-02 19:12:23,643 - INFO - joeynmt.training - Epoch   6, Step:   458900, Batch Loss:     3.038992, Tokens per Sec:     5429, Lr: 0.000033\r\n",
      "2022-08-02 19:12:42,206 - INFO - joeynmt.training - Epoch   6, Step:   459000, Batch Loss:     3.085560, Tokens per Sec:     5445, Lr: 0.000033\r\n",
      "2022-08-02 19:13:17,503 - INFO - joeynmt.helpers - delete models/deen_transformer/456000.ckpt\r\n",
      "2022-08-02 19:13:17,676 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> his <unk> but his <unk> <unk> the <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> was working to <unk> the <unk> <unk>\r\n",
      "2022-08-02 19:13:17,677 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:13:17,678 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:13:17,678 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:13:17,678 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> then <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:13:17,678 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   459000: bleu:   1.31, loss: 49300.5859, ppl:  18.4989, duration: 35.4710s\r\n",
      "2022-08-02 19:13:36,248 - INFO - joeynmt.training - Epoch   6, Step:   459100, Batch Loss:     3.191950, Tokens per Sec:     5577, Lr: 0.000033\r\n",
      "2022-08-02 19:13:54,612 - INFO - joeynmt.training - Epoch   6, Step:   459200, Batch Loss:     3.031353, Tokens per Sec:     5562, Lr: 0.000033\r\n",
      "2022-08-02 19:14:04,438 - INFO - joeynmt.training - Epoch   6: total training loss 2644.03\r\n",
      "2022-08-02 19:14:04,438 - INFO - joeynmt.training - EPOCH 7\r\n",
      "2022-08-02 19:14:13,062 - INFO - joeynmt.training - Epoch   7, Step:   459300, Batch Loss:     2.876210, Tokens per Sec:     5149, Lr: 0.000033\r\n",
      "2022-08-02 19:14:31,537 - INFO - joeynmt.training - Epoch   7, Step:   459400, Batch Loss:     2.976655, Tokens per Sec:     5506, Lr: 0.000033\r\n",
      "2022-08-02 19:14:49,959 - INFO - joeynmt.training - Epoch   7, Step:   459500, Batch Loss:     2.974060, Tokens per Sec:     5384, Lr: 0.000033\r\n",
      "2022-08-02 19:15:08,447 - INFO - joeynmt.training - Epoch   7, Step:   459600, Batch Loss:     3.335193, Tokens per Sec:     5503, Lr: 0.000033\r\n",
      "2022-08-02 19:15:27,061 - INFO - joeynmt.training - Epoch   7, Step:   459700, Batch Loss:     3.106325, Tokens per Sec:     5521, Lr: 0.000033\r\n",
      "2022-08-02 19:15:45,501 - INFO - joeynmt.training - Epoch   7, Step:   459800, Batch Loss:     3.012722, Tokens per Sec:     5412, Lr: 0.000033\r\n",
      "2022-08-02 19:16:03,928 - INFO - joeynmt.training - Epoch   7, Step:   459900, Batch Loss:     2.897507, Tokens per Sec:     5486, Lr: 0.000033\r\n",
      "2022-08-02 19:16:22,363 - INFO - joeynmt.training - Epoch   7, Step:   460000, Batch Loss:     3.342735, Tokens per Sec:     5331, Lr: 0.000033\r\n",
      "2022-08-02 19:16:58,581 - INFO - joeynmt.helpers - delete models/deen_transformer/457000.ckpt\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> but his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:16:58,763 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> two <unk> <unk> <unk>\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> was working to <unk> the <unk> <unk>\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far she opened\r\n",
      "2022-08-02 19:16:58,764 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   460000: bleu:   1.47, loss: 48702.9648, ppl:  17.8561, duration: 36.4008s\r\n",
      "2022-08-02 19:17:17,110 - INFO - joeynmt.training - Epoch   7, Step:   460100, Batch Loss:     3.038702, Tokens per Sec:     5318, Lr: 0.000033\r\n",
      "2022-08-02 19:17:18,679 - INFO - joeynmt.training - Epoch   7: total training loss 2609.58\r\n",
      "2022-08-02 19:17:18,679 - INFO - joeynmt.training - EPOCH 8\r\n",
      "2022-08-02 19:17:35,576 - INFO - joeynmt.training - Epoch   8, Step:   460200, Batch Loss:     3.049854, Tokens per Sec:     5488, Lr: 0.000033\r\n",
      "2022-08-02 19:17:54,014 - INFO - joeynmt.training - Epoch   8, Step:   460300, Batch Loss:     3.172392, Tokens per Sec:     5378, Lr: 0.000033\r\n",
      "2022-08-02 19:18:12,458 - INFO - joeynmt.training - Epoch   8, Step:   460400, Batch Loss:     2.876868, Tokens per Sec:     5429, Lr: 0.000033\r\n",
      "2022-08-02 19:18:30,829 - INFO - joeynmt.training - Epoch   8, Step:   460500, Batch Loss:     2.934796, Tokens per Sec:     5424, Lr: 0.000033\r\n",
      "2022-08-02 19:18:49,381 - INFO - joeynmt.training - Epoch   8, Step:   460600, Batch Loss:     2.952503, Tokens per Sec:     5347, Lr: 0.000033\r\n",
      "2022-08-02 19:19:07,789 - INFO - joeynmt.training - Epoch   8, Step:   460700, Batch Loss:     3.039634, Tokens per Sec:     5596, Lr: 0.000033\r\n",
      "2022-08-02 19:19:26,254 - INFO - joeynmt.training - Epoch   8, Step:   460800, Batch Loss:     2.940445, Tokens per Sec:     5305, Lr: 0.000033\r\n",
      "2022-08-02 19:19:44,697 - INFO - joeynmt.training - Epoch   8, Step:   460900, Batch Loss:     2.983358, Tokens per Sec:     5544, Lr: 0.000033\r\n",
      "2022-08-02 19:19:55,334 - INFO - joeynmt.training - Epoch   8: total training loss 2550.75\r\n",
      "2022-08-02 19:19:55,335 - INFO - joeynmt.training - EPOCH 9\r\n",
      "2022-08-02 19:20:03,230 - INFO - joeynmt.training - Epoch   9, Step:   461000, Batch Loss:     2.903901, Tokens per Sec:     5503, Lr: 0.000033\r\n",
      "2022-08-02 19:20:33,275 - INFO - joeynmt.helpers - delete models/deen_transformer/458000.ckpt\r\n",
      "2022-08-02 19:20:33,446 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:20:33,446 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:20:33,446 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:20:33,446 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> but his <unk> <unk> the <unk> <unk> <unk>\r\n",
      "2022-08-02 19:20:33,446 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> <unk> two <unk> <unk>\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> it was <unk> called <unk>\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far she opened\r\n",
      "2022-08-02 19:20:33,447 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   461000: bleu:   1.92, loss: 48219.6641, ppl:  17.3526, duration: 30.2166s\r\n",
      "2022-08-02 19:20:51,844 - INFO - joeynmt.training - Epoch   9, Step:   461100, Batch Loss:     2.862257, Tokens per Sec:     5420, Lr: 0.000033\r\n",
      "2022-08-02 19:21:10,334 - INFO - joeynmt.training - Epoch   9, Step:   461200, Batch Loss:     3.029787, Tokens per Sec:     5432, Lr: 0.000033\r\n",
      "2022-08-02 19:21:28,717 - INFO - joeynmt.training - Epoch   9, Step:   461300, Batch Loss:     3.141361, Tokens per Sec:     5423, Lr: 0.000033\r\n",
      "2022-08-02 19:21:47,223 - INFO - joeynmt.training - Epoch   9, Step:   461400, Batch Loss:     3.060546, Tokens per Sec:     5370, Lr: 0.000033\r\n",
      "2022-08-02 19:22:05,746 - INFO - joeynmt.training - Epoch   9, Step:   461500, Batch Loss:     2.848212, Tokens per Sec:     5376, Lr: 0.000033\r\n",
      "2022-08-02 19:22:24,347 - INFO - joeynmt.training - Epoch   9, Step:   461600, Batch Loss:     2.894814, Tokens per Sec:     5509, Lr: 0.000033\r\n",
      "2022-08-02 19:22:42,904 - INFO - joeynmt.training - Epoch   9, Step:   461700, Batch Loss:     2.738006, Tokens per Sec:     5437, Lr: 0.000033\r\n",
      "2022-08-02 19:23:01,246 - INFO - joeynmt.training - Epoch   9, Step:   461800, Batch Loss:     2.979571, Tokens per Sec:     5307, Lr: 0.000033\r\n",
      "2022-08-02 19:23:03,579 - INFO - joeynmt.training - Epoch   9: total training loss 2535.79\r\n",
      "2022-08-02 19:23:03,579 - INFO - joeynmt.training - EPOCH 10\r\n",
      "2022-08-02 19:23:19,687 - INFO - joeynmt.training - Epoch  10, Step:   461900, Batch Loss:     2.971300, Tokens per Sec:     5276, Lr: 0.000033\r\n",
      "2022-08-02 19:23:38,033 - INFO - joeynmt.training - Epoch  10, Step:   462000, Batch Loss:     3.039387, Tokens per Sec:     5432, Lr: 0.000033\r\n",
      "2022-08-02 19:24:14,985 - INFO - joeynmt.helpers - delete models/deen_transformer/459000.ckpt\r\n",
      "2022-08-02 19:24:15,146 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:24:15,147 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far she opened\r\n",
      "2022-08-02 19:24:15,148 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   462000: bleu:   1.18, loss: 47647.2031, ppl:  16.7745, duration: 37.1148s\r\n",
      "2022-08-02 19:24:33,568 - INFO - joeynmt.training - Epoch  10, Step:   462100, Batch Loss:     2.929012, Tokens per Sec:     5303, Lr: 0.000033\r\n",
      "2022-08-02 19:24:51,906 - INFO - joeynmt.training - Epoch  10, Step:   462200, Batch Loss:     2.733728, Tokens per Sec:     5335, Lr: 0.000033\r\n",
      "2022-08-02 19:25:10,298 - INFO - joeynmt.training - Epoch  10, Step:   462300, Batch Loss:     2.913745, Tokens per Sec:     5490, Lr: 0.000032\r\n",
      "2022-08-02 19:25:28,989 - INFO - joeynmt.training - Epoch  10, Step:   462400, Batch Loss:     2.870496, Tokens per Sec:     5555, Lr: 0.000032\r\n",
      "2022-08-02 19:25:47,449 - INFO - joeynmt.training - Epoch  10, Step:   462500, Batch Loss:     2.850077, Tokens per Sec:     5467, Lr: 0.000032\r\n",
      "2022-08-02 19:26:05,945 - INFO - joeynmt.training - Epoch  10, Step:   462600, Batch Loss:     3.018058, Tokens per Sec:     5433, Lr: 0.000032\r\n",
      "2022-08-02 19:26:18,469 - INFO - joeynmt.training - Epoch  10: total training loss 2502.44\r\n",
      "2022-08-02 19:26:18,470 - INFO - joeynmt.training - EPOCH 11\r\n",
      "2022-08-02 19:26:24,568 - INFO - joeynmt.training - Epoch  11, Step:   462700, Batch Loss:     2.691857, Tokens per Sec:     5407, Lr: 0.000032\r\n",
      "2022-08-02 19:26:42,960 - INFO - joeynmt.training - Epoch  11, Step:   462800, Batch Loss:     3.309769, Tokens per Sec:     5473, Lr: 0.000032\r\n",
      "2022-08-02 19:27:01,169 - INFO - joeynmt.training - Epoch  11, Step:   462900, Batch Loss:     2.900186, Tokens per Sec:     5318, Lr: 0.000032\r\n",
      "2022-08-02 19:27:19,483 - INFO - joeynmt.training - Epoch  11, Step:   463000, Batch Loss:     2.866424, Tokens per Sec:     5429, Lr: 0.000032\r\n",
      "2022-08-02 19:27:56,268 - INFO - joeynmt.helpers - delete models/deen_transformer/460000.ckpt\r\n",
      "2022-08-02 19:27:56,436 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:27:56,436 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:27:56,436 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> <unk> two <unk> <unk> <unk>\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:27:56,437 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:27:56,438 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 19:27:56,438 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   463000: bleu:   1.22, loss: 47298.9414, ppl:  16.4323, duration: 36.9539s\r\n",
      "2022-08-02 19:28:14,951 - INFO - joeynmt.training - Epoch  11, Step:   463100, Batch Loss:     2.775026, Tokens per Sec:     5313, Lr: 0.000032\r\n",
      "2022-08-02 19:28:33,273 - INFO - joeynmt.training - Epoch  11, Step:   463200, Batch Loss:     3.001104, Tokens per Sec:     5553, Lr: 0.000032\r\n",
      "2022-08-02 19:28:51,583 - INFO - joeynmt.training - Epoch  11, Step:   463300, Batch Loss:     2.855787, Tokens per Sec:     5615, Lr: 0.000032\r\n",
      "2022-08-02 19:29:09,954 - INFO - joeynmt.training - Epoch  11, Step:   463400, Batch Loss:     2.785923, Tokens per Sec:     5472, Lr: 0.000032\r\n",
      "2022-08-02 19:29:28,352 - INFO - joeynmt.training - Epoch  11, Step:   463500, Batch Loss:     2.980788, Tokens per Sec:     5359, Lr: 0.000032\r\n",
      "2022-08-02 19:29:32,783 - INFO - joeynmt.training - Epoch  11: total training loss 2478.74\r\n",
      "2022-08-02 19:29:32,784 - INFO - joeynmt.training - EPOCH 12\r\n",
      "2022-08-02 19:29:46,790 - INFO - joeynmt.training - Epoch  12, Step:   463600, Batch Loss:     2.722781, Tokens per Sec:     5376, Lr: 0.000032\r\n",
      "2022-08-02 19:30:05,307 - INFO - joeynmt.training - Epoch  12, Step:   463700, Batch Loss:     2.869694, Tokens per Sec:     5358, Lr: 0.000032\r\n",
      "2022-08-02 19:30:23,708 - INFO - joeynmt.training - Epoch  12, Step:   463800, Batch Loss:     2.871502, Tokens per Sec:     5373, Lr: 0.000032\r\n",
      "2022-08-02 19:30:42,309 - INFO - joeynmt.training - Epoch  12, Step:   463900, Batch Loss:     2.779139, Tokens per Sec:     5465, Lr: 0.000032\r\n",
      "2022-08-02 19:31:00,672 - INFO - joeynmt.training - Epoch  12, Step:   464000, Batch Loss:     2.797941, Tokens per Sec:     5375, Lr: 0.000032\r\n",
      "2022-08-02 19:31:37,458 - INFO - joeynmt.helpers - delete models/deen_transformer/461000.ckpt\r\n",
      "2022-08-02 19:31:37,621 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> but his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> <unk> two <unk> <unk> <unk>\r\n",
      "2022-08-02 19:31:37,622 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> work it was so <unk> called <unk>\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far her <unk> open\r\n",
      "2022-08-02 19:31:37,623 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   464000: bleu:   1.68, loss: 47031.5977, ppl:  16.1744, duration: 36.9513s\r\n",
      "2022-08-02 19:31:56,138 - INFO - joeynmt.training - Epoch  12, Step:   464100, Batch Loss:     3.048372, Tokens per Sec:     5519, Lr: 0.000032\r\n",
      "2022-08-02 19:32:14,547 - INFO - joeynmt.training - Epoch  12, Step:   464200, Batch Loss:     3.018090, Tokens per Sec:     5582, Lr: 0.000032\r\n",
      "2022-08-02 19:32:32,831 - INFO - joeynmt.training - Epoch  12, Step:   464300, Batch Loss:     2.794682, Tokens per Sec:     5483, Lr: 0.000032\r\n",
      "2022-08-02 19:32:46,654 - INFO - joeynmt.training - Epoch  12: total training loss 2433.11\r\n",
      "2022-08-02 19:32:46,655 - INFO - joeynmt.training - EPOCH 13\r\n",
      "2022-08-02 19:32:51,423 - INFO - joeynmt.training - Epoch  13, Step:   464400, Batch Loss:     2.876570, Tokens per Sec:     5106, Lr: 0.000032\r\n",
      "2022-08-02 19:33:09,822 - INFO - joeynmt.training - Epoch  13, Step:   464500, Batch Loss:     2.776922, Tokens per Sec:     5429, Lr: 0.000032\r\n",
      "2022-08-02 19:33:28,323 - INFO - joeynmt.training - Epoch  13, Step:   464600, Batch Loss:     2.797584, Tokens per Sec:     5336, Lr: 0.000032\r\n",
      "2022-08-02 19:33:46,521 - INFO - joeynmt.training - Epoch  13, Step:   464700, Batch Loss:     2.822052, Tokens per Sec:     5478, Lr: 0.000032\r\n",
      "2022-08-02 19:34:05,042 - INFO - joeynmt.training - Epoch  13, Step:   464800, Batch Loss:     2.813667, Tokens per Sec:     5524, Lr: 0.000032\r\n",
      "2022-08-02 19:34:23,454 - INFO - joeynmt.training - Epoch  13, Step:   464900, Batch Loss:     2.798975, Tokens per Sec:     5546, Lr: 0.000032\r\n",
      "2022-08-02 19:34:41,816 - INFO - joeynmt.training - Epoch  13, Step:   465000, Batch Loss:     2.821934, Tokens per Sec:     5467, Lr: 0.000032\r\n",
      "2022-08-02 19:35:18,592 - INFO - joeynmt.helpers - delete models/deen_transformer/462000.ckpt\r\n",
      "2022-08-02 19:35:18,751 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> <unk> two <unk> <unk> <unk>\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:35:18,752 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:35:18,753 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:35:18,753 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:35:18,753 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:35:18,753 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> <unk> to see how far her <unk> open\r\n",
      "2022-08-02 19:35:18,753 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step   465000: bleu:   1.58, loss: 46727.2383, ppl:  15.8857, duration: 36.9367s\r\n",
      "2022-08-02 19:35:37,064 - INFO - joeynmt.training - Epoch  13, Step:   465100, Batch Loss:     3.004786, Tokens per Sec:     5513, Lr: 0.000032\r\n",
      "2022-08-02 19:35:55,394 - INFO - joeynmt.training - Epoch  13, Step:   465200, Batch Loss:     2.890287, Tokens per Sec:     5438, Lr: 0.000032\r\n",
      "2022-08-02 19:36:00,286 - INFO - joeynmt.training - Epoch  13: total training loss 2412.05\r\n",
      "2022-08-02 19:36:00,286 - INFO - joeynmt.training - EPOCH 14\r\n",
      "2022-08-02 19:36:13,844 - INFO - joeynmt.training - Epoch  14, Step:   465300, Batch Loss:     2.865102, Tokens per Sec:     5306, Lr: 0.000032\r\n",
      "2022-08-02 19:36:32,378 - INFO - joeynmt.training - Epoch  14, Step:   465400, Batch Loss:     2.728154, Tokens per Sec:     5622, Lr: 0.000032\r\n",
      "2022-08-02 19:36:50,660 - INFO - joeynmt.training - Epoch  14, Step:   465500, Batch Loss:     2.890401, Tokens per Sec:     5375, Lr: 0.000032\r\n",
      "2022-08-02 19:37:08,845 - INFO - joeynmt.training - Epoch  14, Step:   465600, Batch Loss:     2.934989, Tokens per Sec:     5617, Lr: 0.000032\r\n",
      "2022-08-02 19:37:27,192 - INFO - joeynmt.training - Epoch  14, Step:   465700, Batch Loss:     2.644540, Tokens per Sec:     5622, Lr: 0.000032\r\n",
      "2022-08-02 19:37:45,520 - INFO - joeynmt.training - Epoch  14, Step:   465800, Batch Loss:     2.942452, Tokens per Sec:     5406, Lr: 0.000032\r\n",
      "2022-08-02 19:38:03,923 - INFO - joeynmt.training - Epoch  14, Step:   465900, Batch Loss:     2.749503, Tokens per Sec:     5543, Lr: 0.000032\r\n",
      "2022-08-02 19:38:22,348 - INFO - joeynmt.training - Epoch  14, Step:   466000, Batch Loss:     2.872537, Tokens per Sec:     5490, Lr: 0.000032\r\n",
      "2022-08-02 19:38:59,398 - INFO - joeynmt.helpers - delete models/deen_transformer/463000.ckpt\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:38:59,566 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> <unk> to see how far her <unk> open\r\n",
      "2022-08-02 19:38:59,567 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step   466000: bleu:   1.91, loss: 46562.3281, ppl:  15.7314, duration: 37.2184s\r\n",
      "2022-08-02 19:39:13,440 - INFO - joeynmt.training - Epoch  14: total training loss 2380.06\r\n",
      "2022-08-02 19:39:13,441 - INFO - joeynmt.training - EPOCH 15\r\n",
      "2022-08-02 19:39:18,056 - INFO - joeynmt.training - Epoch  15, Step:   466100, Batch Loss:     2.714546, Tokens per Sec:     5257, Lr: 0.000032\r\n",
      "2022-08-02 19:39:36,447 - INFO - joeynmt.training - Epoch  15, Step:   466200, Batch Loss:     2.687036, Tokens per Sec:     5469, Lr: 0.000032\r\n",
      "2022-08-02 19:39:54,827 - INFO - joeynmt.training - Epoch  15, Step:   466300, Batch Loss:     2.626258, Tokens per Sec:     5387, Lr: 0.000032\r\n",
      "2022-08-02 19:40:13,038 - INFO - joeynmt.training - Epoch  15, Step:   466400, Batch Loss:     2.692832, Tokens per Sec:     5330, Lr: 0.000032\r\n",
      "2022-08-02 19:40:31,292 - INFO - joeynmt.training - Epoch  15, Step:   466500, Batch Loss:     2.841319, Tokens per Sec:     5517, Lr: 0.000032\r\n",
      "2022-08-02 19:40:49,871 - INFO - joeynmt.training - Epoch  15, Step:   466600, Batch Loss:     2.644408, Tokens per Sec:     5546, Lr: 0.000032\r\n",
      "2022-08-02 19:41:08,127 - INFO - joeynmt.training - Epoch  15, Step:   466700, Batch Loss:     3.109140, Tokens per Sec:     5431, Lr: 0.000032\r\n",
      "2022-08-02 19:41:26,680 - INFO - joeynmt.training - Epoch  15, Step:   466800, Batch Loss:     2.912109, Tokens per Sec:     5559, Lr: 0.000032\r\n",
      "2022-08-02 19:41:45,156 - INFO - joeynmt.training - Epoch  15, Step:   466900, Batch Loss:     2.714377, Tokens per Sec:     5228, Lr: 0.000032\r\n",
      "2022-08-02 19:41:50,942 - INFO - joeynmt.training - Epoch  15: total training loss 2375.77\r\n",
      "2022-08-02 19:41:50,943 - INFO - joeynmt.training - EPOCH 16\r\n",
      "2022-08-02 19:42:03,676 - INFO - joeynmt.training - Epoch  16, Step:   467000, Batch Loss:     2.728000, Tokens per Sec:     5446, Lr: 0.000032\r\n",
      "2022-08-02 19:42:40,406 - INFO - joeynmt.helpers - delete models/deen_transformer/464000.ckpt\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk>\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:42:40,576 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> that <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> then to see how far her <unk> open\r\n",
      "2022-08-02 19:42:40,577 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step   467000: bleu:   1.95, loss: 46484.9492, ppl:  15.6595, duration: 36.9012s\r\n",
      "2022-08-02 19:42:58,849 - INFO - joeynmt.training - Epoch  16, Step:   467100, Batch Loss:     2.791119, Tokens per Sec:     5351, Lr: 0.000032\r\n",
      "2022-08-02 19:43:17,113 - INFO - joeynmt.training - Epoch  16, Step:   467200, Batch Loss:     2.750963, Tokens per Sec:     5475, Lr: 0.000032\r\n",
      "2022-08-02 19:43:35,482 - INFO - joeynmt.training - Epoch  16, Step:   467300, Batch Loss:     2.823424, Tokens per Sec:     5479, Lr: 0.000032\r\n",
      "2022-08-02 19:43:53,787 - INFO - joeynmt.training - Epoch  16, Step:   467400, Batch Loss:     2.869083, Tokens per Sec:     5378, Lr: 0.000032\r\n",
      "2022-08-02 19:44:12,090 - INFO - joeynmt.training - Epoch  16, Step:   467500, Batch Loss:     2.913626, Tokens per Sec:     5475, Lr: 0.000032\r\n",
      "2022-08-02 19:44:30,504 - INFO - joeynmt.training - Epoch  16, Step:   467600, Batch Loss:     2.733955, Tokens per Sec:     5364, Lr: 0.000032\r\n",
      "2022-08-02 19:44:48,852 - INFO - joeynmt.training - Epoch  16, Step:   467700, Batch Loss:     2.844191, Tokens per Sec:     5415, Lr: 0.000032\r\n",
      "2022-08-02 19:45:05,349 - INFO - joeynmt.training - Epoch  16: total training loss 2360.12\r\n",
      "2022-08-02 19:45:05,349 - INFO - joeynmt.training - EPOCH 17\r\n",
      "2022-08-02 19:45:07,354 - INFO - joeynmt.training - Epoch  17, Step:   467800, Batch Loss:     2.842543, Tokens per Sec:     4928, Lr: 0.000032\r\n",
      "2022-08-02 19:45:25,776 - INFO - joeynmt.training - Epoch  17, Step:   467900, Batch Loss:     2.732108, Tokens per Sec:     5491, Lr: 0.000032\r\n",
      "2022-08-02 19:45:44,025 - INFO - joeynmt.training - Epoch  17, Step:   468000, Batch Loss:     2.962000, Tokens per Sec:     5414, Lr: 0.000032\r\n",
      "2022-08-02 19:46:20,983 - INFO - joeynmt.helpers - delete models/deen_transformer/465000.ckpt\r\n",
      "2022-08-02 19:46:21,149 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:46:21,149 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:46:21,149 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:46:21,149 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:46:21,150 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:46:21,151 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 19:46:21,151 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step   468000: bleu:   1.88, loss: 46357.1055, ppl:  15.5415, duration: 37.1256s\r\n",
      "2022-08-02 19:46:39,232 - INFO - joeynmt.training - Epoch  17, Step:   468100, Batch Loss:     2.691042, Tokens per Sec:     5423, Lr: 0.000032\r\n",
      "2022-08-02 19:46:57,690 - INFO - joeynmt.training - Epoch  17, Step:   468200, Batch Loss:     2.565908, Tokens per Sec:     5444, Lr: 0.000032\r\n",
      "2022-08-02 19:47:16,048 - INFO - joeynmt.training - Epoch  17, Step:   468300, Batch Loss:     2.736302, Tokens per Sec:     5495, Lr: 0.000032\r\n",
      "2022-08-02 19:47:34,216 - INFO - joeynmt.training - Epoch  17, Step:   468400, Batch Loss:     2.869875, Tokens per Sec:     5485, Lr: 0.000032\r\n",
      "2022-08-02 19:47:52,590 - INFO - joeynmt.training - Epoch  17, Step:   468500, Batch Loss:     2.934945, Tokens per Sec:     5583, Lr: 0.000032\r\n",
      "2022-08-02 19:48:11,011 - INFO - joeynmt.training - Epoch  17, Step:   468600, Batch Loss:     2.569404, Tokens per Sec:     5468, Lr: 0.000032\r\n",
      "2022-08-02 19:48:19,441 - INFO - joeynmt.training - Epoch  17: total training loss 2335.17\r\n",
      "2022-08-02 19:48:19,441 - INFO - joeynmt.training - EPOCH 18\r\n",
      "2022-08-02 19:48:29,250 - INFO - joeynmt.training - Epoch  18, Step:   468700, Batch Loss:     2.639866, Tokens per Sec:     5500, Lr: 0.000032\r\n",
      "2022-08-02 19:48:47,540 - INFO - joeynmt.training - Epoch  18, Step:   468800, Batch Loss:     2.824602, Tokens per Sec:     5532, Lr: 0.000032\r\n",
      "2022-08-02 19:49:05,704 - INFO - joeynmt.training - Epoch  18, Step:   468900, Batch Loss:     2.648685, Tokens per Sec:     5369, Lr: 0.000032\r\n",
      "2022-08-02 19:49:23,977 - INFO - joeynmt.training - Epoch  18, Step:   469000, Batch Loss:     2.790502, Tokens per Sec:     5497, Lr: 0.000032\r\n",
      "2022-08-02 19:50:00,954 - INFO - joeynmt.helpers - delete models/deen_transformer/466000.ckpt\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> <unk> his <unk> <unk> <unk> the <unk> <unk> <unk>\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> work <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 19:50:01,122 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> it was so <unk> called <unk> ping\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> to see how far her <unk> open\r\n",
      "2022-08-02 19:50:01,123 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step   469000: bleu:   2.04, loss: 46207.6836, ppl:  15.4046, duration: 37.1463s\r\n",
      "2022-08-02 19:50:19,558 - INFO - joeynmt.training - Epoch  18, Step:   469100, Batch Loss:     2.719864, Tokens per Sec:     5422, Lr: 0.000032\r\n",
      "2022-08-02 19:50:38,252 - INFO - joeynmt.training - Epoch  18, Step:   469200, Batch Loss:     2.765308, Tokens per Sec:     5619, Lr: 0.000032\r\n",
      "2022-08-02 19:50:56,682 - INFO - joeynmt.training - Epoch  18, Step:   469300, Batch Loss:     2.608580, Tokens per Sec:     5408, Lr: 0.000032\r\n",
      "2022-08-02 19:51:15,123 - INFO - joeynmt.training - Epoch  18, Step:   469400, Batch Loss:     2.767419, Tokens per Sec:     5358, Lr: 0.000032\r\n",
      "2022-08-02 19:51:33,514 - INFO - joeynmt.training - Epoch  18, Step:   469500, Batch Loss:     2.582782, Tokens per Sec:     5360, Lr: 0.000032\r\n",
      "2022-08-02 19:51:33,517 - INFO - joeynmt.training - Epoch  18: total training loss 2304.16\r\n",
      "2022-08-02 19:51:33,517 - INFO - joeynmt.training - EPOCH 19\r\n",
      "2022-08-02 19:51:52,011 - INFO - joeynmt.training - Epoch  19, Step:   469600, Batch Loss:     2.624393, Tokens per Sec:     5448, Lr: 0.000032\r\n",
      "2022-08-02 19:52:10,295 - INFO - joeynmt.training - Epoch  19, Step:   469700, Batch Loss:     2.560813, Tokens per Sec:     5365, Lr: 0.000032\r\n",
      "2022-08-02 19:52:28,680 - INFO - joeynmt.training - Epoch  19, Step:   469800, Batch Loss:     2.549066, Tokens per Sec:     5462, Lr: 0.000032\r\n",
      "2022-08-02 19:52:47,020 - INFO - joeynmt.training - Epoch  19, Step:   469900, Batch Loss:     2.900442, Tokens per Sec:     5415, Lr: 0.000032\r\n",
      "2022-08-02 19:53:05,439 - INFO - joeynmt.training - Epoch  19, Step:   470000, Batch Loss:     2.625374, Tokens per Sec:     5323, Lr: 0.000032\r\n",
      "2022-08-02 19:53:42,363 - INFO - joeynmt.helpers - delete models/deen_transformer/467000.ckpt\r\n",
      "2022-08-02 19:53:42,536 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:53:42,536 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:53:42,536 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> it was so <unk> called <unk> ping\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:53:42,537 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 19:53:42,538 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step   470000: bleu:   1.92, loss: 46202.0898, ppl:  15.3995, duration: 37.0978s\r\n",
      "2022-08-02 19:54:00,974 - INFO - joeynmt.training - Epoch  19, Step:   470100, Batch Loss:     2.649972, Tokens per Sec:     5624, Lr: 0.000032\r\n",
      "2022-08-02 19:54:19,366 - INFO - joeynmt.training - Epoch  19, Step:   470200, Batch Loss:     2.633235, Tokens per Sec:     5399, Lr: 0.000032\r\n",
      "2022-08-02 19:54:37,672 - INFO - joeynmt.training - Epoch  19, Step:   470300, Batch Loss:     2.828552, Tokens per Sec:     5531, Lr: 0.000032\r\n",
      "2022-08-02 19:54:47,255 - INFO - joeynmt.training - Epoch  19: total training loss 2281.37\r\n",
      "2022-08-02 19:54:47,255 - INFO - joeynmt.training - EPOCH 20\r\n",
      "2022-08-02 19:54:55,930 - INFO - joeynmt.training - Epoch  20, Step:   470400, Batch Loss:     2.708344, Tokens per Sec:     5266, Lr: 0.000032\r\n",
      "2022-08-02 19:55:14,383 - INFO - joeynmt.training - Epoch  20, Step:   470500, Batch Loss:     2.598808, Tokens per Sec:     5423, Lr: 0.000032\r\n",
      "2022-08-02 19:55:32,755 - INFO - joeynmt.training - Epoch  20, Step:   470600, Batch Loss:     2.570775, Tokens per Sec:     5484, Lr: 0.000032\r\n",
      "2022-08-02 19:55:50,799 - INFO - joeynmt.training - Epoch  20, Step:   470700, Batch Loss:     2.582306, Tokens per Sec:     5399, Lr: 0.000032\r\n",
      "2022-08-02 19:56:09,058 - INFO - joeynmt.training - Epoch  20, Step:   470800, Batch Loss:     2.669030, Tokens per Sec:     5544, Lr: 0.000032\r\n",
      "2022-08-02 19:56:27,521 - INFO - joeynmt.training - Epoch  20, Step:   470900, Batch Loss:     2.489970, Tokens per Sec:     5504, Lr: 0.000032\r\n",
      "2022-08-02 19:56:45,908 - INFO - joeynmt.training - Epoch  20, Step:   471000, Batch Loss:     2.500596, Tokens per Sec:     5408, Lr: 0.000032\r\n",
      "2022-08-02 19:57:22,887 - INFO - joeynmt.helpers - delete models/deen_transformer/468000.ckpt\r\n",
      "2022-08-02 19:57:23,058 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 19:57:23,058 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 19:57:23,058 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 19:57:23,058 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 19:57:23,058 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> work <unk> <unk> about the <unk> of a <unk> that <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 19:57:23,059 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step   471000: bleu:   2.06, loss: 46051.5508, ppl:  15.2629, duration: 37.1511s\r\n",
      "2022-08-02 19:57:41,575 - INFO - joeynmt.training - Epoch  20, Step:   471100, Batch Loss:     2.694701, Tokens per Sec:     5542, Lr: 0.000032\r\n",
      "2022-08-02 19:58:03,194 - INFO - joeynmt.training - Epoch  20, Step:   471200, Batch Loss:     2.706927, Tokens per Sec:     4610, Lr: 0.000032\r\n",
      "2022-08-02 19:58:04,464 - INFO - joeynmt.training - Epoch  20: total training loss 2266.61\r\n",
      "2022-08-02 19:58:04,464 - INFO - joeynmt.training - EPOCH 21\r\n",
      "2022-08-02 19:58:21,563 - INFO - joeynmt.training - Epoch  21, Step:   471300, Batch Loss:     2.612984, Tokens per Sec:     5347, Lr: 0.000032\r\n",
      "2022-08-02 19:58:39,833 - INFO - joeynmt.training - Epoch  21, Step:   471400, Batch Loss:     2.655685, Tokens per Sec:     5418, Lr: 0.000032\r\n",
      "2022-08-02 19:58:58,157 - INFO - joeynmt.training - Epoch  21, Step:   471500, Batch Loss:     2.703609, Tokens per Sec:     5447, Lr: 0.000032\r\n",
      "2022-08-02 19:59:16,462 - INFO - joeynmt.training - Epoch  21, Step:   471600, Batch Loss:     2.351928, Tokens per Sec:     5313, Lr: 0.000032\r\n",
      "2022-08-02 19:59:34,918 - INFO - joeynmt.training - Epoch  21, Step:   471700, Batch Loss:     2.566694, Tokens per Sec:     5484, Lr: 0.000032\r\n",
      "2022-08-02 19:59:53,523 - INFO - joeynmt.training - Epoch  21, Step:   471800, Batch Loss:     2.686973, Tokens per Sec:     5561, Lr: 0.000032\r\n",
      "2022-08-02 20:00:11,682 - INFO - joeynmt.training - Epoch  21, Step:   471900, Batch Loss:     2.648078, Tokens per Sec:     5448, Lr: 0.000032\r\n",
      "2022-08-02 20:00:30,331 - INFO - joeynmt.training - Epoch  21, Step:   472000, Batch Loss:     2.618418, Tokens per Sec:     5421, Lr: 0.000032\r\n",
      "2022-08-02 20:01:09,295 - INFO - joeynmt.helpers - delete models/deen_transformer/469000.ckpt\r\n",
      "2022-08-02 20:01:09,460 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:01:09,460 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:01:09,460 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:01:09,460 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:01:09,460 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 20:01:09,461 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step   472000: bleu:   1.78, loss: 45945.2031, ppl:  15.1672, duration: 39.1298s\r\n",
      "2022-08-02 20:01:21,008 - INFO - joeynmt.training - Epoch  21: total training loss 2256.39\r\n",
      "2022-08-02 20:01:21,008 - INFO - joeynmt.training - EPOCH 22\r\n",
      "2022-08-02 20:01:27,643 - INFO - joeynmt.training - Epoch  22, Step:   472100, Batch Loss:     2.367015, Tokens per Sec:     5182, Lr: 0.000032\r\n",
      "2022-08-02 20:01:45,954 - INFO - joeynmt.training - Epoch  22, Step:   472200, Batch Loss:     2.500157, Tokens per Sec:     5644, Lr: 0.000032\r\n",
      "2022-08-02 20:02:04,349 - INFO - joeynmt.training - Epoch  22, Step:   472300, Batch Loss:     2.692248, Tokens per Sec:     5359, Lr: 0.000032\r\n",
      "2022-08-02 20:02:22,858 - INFO - joeynmt.training - Epoch  22, Step:   472400, Batch Loss:     2.455736, Tokens per Sec:     5532, Lr: 0.000032\r\n",
      "2022-08-02 20:02:41,263 - INFO - joeynmt.training - Epoch  22, Step:   472500, Batch Loss:     2.378959, Tokens per Sec:     5591, Lr: 0.000032\r\n",
      "2022-08-02 20:02:59,543 - INFO - joeynmt.training - Epoch  22, Step:   472600, Batch Loss:     2.501412, Tokens per Sec:     5396, Lr: 0.000032\r\n",
      "2022-08-02 20:03:18,024 - INFO - joeynmt.training - Epoch  22, Step:   472700, Batch Loss:     2.691762, Tokens per Sec:     5398, Lr: 0.000032\r\n",
      "2022-08-02 20:03:36,234 - INFO - joeynmt.training - Epoch  22, Step:   472800, Batch Loss:     2.643662, Tokens per Sec:     5439, Lr: 0.000032\r\n",
      "2022-08-02 20:03:54,501 - INFO - joeynmt.training - Epoch  22, Step:   472900, Batch Loss:     2.812710, Tokens per Sec:     5386, Lr: 0.000032\r\n",
      "2022-08-02 20:03:58,336 - INFO - joeynmt.training - Epoch  22: total training loss 2231.43\r\n",
      "2022-08-02 20:03:58,336 - INFO - joeynmt.training - EPOCH 23\r\n",
      "2022-08-02 20:04:13,577 - INFO - joeynmt.training - Epoch  23, Step:   473000, Batch Loss:     2.517040, Tokens per Sec:     5479, Lr: 0.000032\r\n",
      "2022-08-02 20:04:50,943 - INFO - joeynmt.helpers - delete models/deen_transformer/470000.ckpt\r\n",
      "2022-08-02 20:04:51,114 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:04:51,114 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:04:51,114 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:04:51,114 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:04:51,114 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> of a <unk> that <unk> two <unk> at the same time\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:04:51,115 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\r\n",
      "2022-08-02 20:04:51,116 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:04:51,116 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:04:51,116 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:04:51,116 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 20:04:51,116 - INFO - joeynmt.training - Validation result (greedy) at epoch  23, step   473000: bleu:   2.16, loss: 46015.1953, ppl:  15.2301, duration: 37.5392s\r\n",
      "2022-08-02 20:05:09,404 - INFO - joeynmt.training - Epoch  23, Step:   473100, Batch Loss:     2.389420, Tokens per Sec:     5404, Lr: 0.000032\r\n",
      "2022-08-02 20:05:27,587 - INFO - joeynmt.training - Epoch  23, Step:   473200, Batch Loss:     2.591918, Tokens per Sec:     5538, Lr: 0.000032\r\n",
      "2022-08-02 20:05:46,060 - INFO - joeynmt.training - Epoch  23, Step:   473300, Batch Loss:     2.725181, Tokens per Sec:     5485, Lr: 0.000032\r\n",
      "2022-08-02 20:06:04,510 - INFO - joeynmt.training - Epoch  23, Step:   473400, Batch Loss:     2.638769, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:06:22,697 - INFO - joeynmt.training - Epoch  23, Step:   473500, Batch Loss:     2.770862, Tokens per Sec:     5424, Lr: 0.000032\r\n",
      "2022-08-02 20:06:41,129 - INFO - joeynmt.training - Epoch  23, Step:   473600, Batch Loss:     2.604052, Tokens per Sec:     5452, Lr: 0.000032\r\n",
      "2022-08-02 20:06:59,472 - INFO - joeynmt.training - Epoch  23, Step:   473700, Batch Loss:     2.616057, Tokens per Sec:     5448, Lr: 0.000032\r\n",
      "2022-08-02 20:07:12,333 - INFO - joeynmt.training - Epoch  23: total training loss 2209.63\r\n",
      "2022-08-02 20:07:12,333 - INFO - joeynmt.training - EPOCH 24\r\n",
      "2022-08-02 20:07:17,748 - INFO - joeynmt.training - Epoch  24, Step:   473800, Batch Loss:     2.765493, Tokens per Sec:     5373, Lr: 0.000032\r\n",
      "2022-08-02 20:07:36,132 - INFO - joeynmt.training - Epoch  24, Step:   473900, Batch Loss:     2.697483, Tokens per Sec:     5298, Lr: 0.000032\r\n",
      "2022-08-02 20:07:54,633 - INFO - joeynmt.training - Epoch  24, Step:   474000, Batch Loss:     2.619233, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:08:31,619 - INFO - joeynmt.helpers - delete models/deen_transformer/471000.ckpt\r\n",
      "2022-08-02 20:08:31,787 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:08:31,787 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:08:31,788 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:08:31,789 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 20:08:31,789 - INFO - joeynmt.training - Validation result (greedy) at epoch  24, step   474000: bleu:   1.70, loss: 46049.4141, ppl:  15.2610, duration: 37.1553s\r\n",
      "2022-08-02 20:08:50,224 - INFO - joeynmt.training - Epoch  24, Step:   474100, Batch Loss:     2.524167, Tokens per Sec:     5439, Lr: 0.000032\r\n",
      "2022-08-02 20:09:08,522 - INFO - joeynmt.training - Epoch  24, Step:   474200, Batch Loss:     2.594337, Tokens per Sec:     5485, Lr: 0.000032\r\n",
      "2022-08-02 20:09:26,928 - INFO - joeynmt.training - Epoch  24, Step:   474300, Batch Loss:     2.729821, Tokens per Sec:     5525, Lr: 0.000032\r\n",
      "2022-08-02 20:09:45,238 - INFO - joeynmt.training - Epoch  24, Step:   474400, Batch Loss:     2.617985, Tokens per Sec:     5539, Lr: 0.000032\r\n",
      "2022-08-02 20:10:03,542 - INFO - joeynmt.training - Epoch  24, Step:   474500, Batch Loss:     2.543462, Tokens per Sec:     5557, Lr: 0.000032\r\n",
      "2022-08-02 20:10:21,980 - INFO - joeynmt.training - Epoch  24, Step:   474600, Batch Loss:     2.603678, Tokens per Sec:     5379, Lr: 0.000032\r\n",
      "2022-08-02 20:10:25,783 - INFO - joeynmt.training - Epoch  24: total training loss 2183.45\r\n",
      "2022-08-02 20:10:25,783 - INFO - joeynmt.training - EPOCH 25\r\n",
      "2022-08-02 20:10:40,468 - INFO - joeynmt.training - Epoch  25, Step:   474700, Batch Loss:     2.428853, Tokens per Sec:     5493, Lr: 0.000032\r\n",
      "2022-08-02 20:10:58,897 - INFO - joeynmt.training - Epoch  25, Step:   474800, Batch Loss:     2.681077, Tokens per Sec:     5396, Lr: 0.000032\r\n",
      "2022-08-02 20:11:17,107 - INFO - joeynmt.training - Epoch  25, Step:   474900, Batch Loss:     2.482816, Tokens per Sec:     5403, Lr: 0.000032\r\n",
      "2022-08-02 20:11:35,736 - INFO - joeynmt.training - Epoch  25, Step:   475000, Batch Loss:     2.463951, Tokens per Sec:     5541, Lr: 0.000032\r\n",
      "2022-08-02 20:12:12,767 - INFO - joeynmt.helpers - delete models/deen_transformer/474000.ckpt\r\n",
      "2022-08-02 20:12:12,934 - INFO - joeynmt.helpers - delete /home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/474000.ckpt\r\n",
      "2022-08-02 20:12:12,935 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/474000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/474000.ckpt')\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:12:12,938 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> in the <unk> <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 20:12:12,939 - INFO - joeynmt.training - Validation result (greedy) at epoch  25, step   475000: bleu:   1.71, loss: 45993.6719, ppl:  15.2107, duration: 37.2027s\r\n",
      "2022-08-02 20:12:31,344 - INFO - joeynmt.training - Epoch  25, Step:   475100, Batch Loss:     2.529789, Tokens per Sec:     5503, Lr: 0.000032\r\n",
      "2022-08-02 20:12:49,782 - INFO - joeynmt.training - Epoch  25, Step:   475200, Batch Loss:     2.527558, Tokens per Sec:     5561, Lr: 0.000032\r\n",
      "2022-08-02 20:13:08,311 - INFO - joeynmt.training - Epoch  25, Step:   475300, Batch Loss:     2.672601, Tokens per Sec:     5463, Lr: 0.000032\r\n",
      "2022-08-02 20:13:26,467 - INFO - joeynmt.training - Epoch  25, Step:   475400, Batch Loss:     2.656858, Tokens per Sec:     5326, Lr: 0.000032\r\n",
      "2022-08-02 20:13:39,783 - INFO - joeynmt.training - Epoch  25: total training loss 2174.98\r\n",
      "2022-08-02 20:13:39,783 - INFO - joeynmt.training - EPOCH 26\r\n",
      "2022-08-02 20:13:44,633 - INFO - joeynmt.training - Epoch  26, Step:   475500, Batch Loss:     2.516235, Tokens per Sec:     5245, Lr: 0.000032\r\n",
      "2022-08-02 20:14:02,894 - INFO - joeynmt.training - Epoch  26, Step:   475600, Batch Loss:     2.662411, Tokens per Sec:     5560, Lr: 0.000032\r\n",
      "2022-08-02 20:14:21,389 - INFO - joeynmt.training - Epoch  26, Step:   475700, Batch Loss:     2.502264, Tokens per Sec:     5419, Lr: 0.000032\r\n",
      "2022-08-02 20:14:39,649 - INFO - joeynmt.training - Epoch  26, Step:   475800, Batch Loss:     2.606834, Tokens per Sec:     5446, Lr: 0.000032\r\n",
      "2022-08-02 20:14:57,904 - INFO - joeynmt.training - Epoch  26, Step:   475900, Batch Loss:     2.448754, Tokens per Sec:     5385, Lr: 0.000032\r\n",
      "2022-08-02 20:15:16,326 - INFO - joeynmt.training - Epoch  26, Step:   476000, Batch Loss:     2.764003, Tokens per Sec:     5436, Lr: 0.000032\r\n",
      "2022-08-02 20:15:53,349 - INFO - joeynmt.helpers - delete models/deen_transformer/473000.ckpt\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:15:53,527 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> then to see how far her <unk> opened\r\n",
      "2022-08-02 20:15:53,528 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step   476000: bleu:   1.71, loss: 45969.4492, ppl:  15.1890, duration: 37.2015s\r\n",
      "2022-08-02 20:16:11,816 - INFO - joeynmt.training - Epoch  26, Step:   476100, Batch Loss:     2.436421, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:16:30,041 - INFO - joeynmt.training - Epoch  26, Step:   476200, Batch Loss:     2.596724, Tokens per Sec:     5374, Lr: 0.000032\r\n",
      "2022-08-02 20:16:48,472 - INFO - joeynmt.training - Epoch  26, Step:   476300, Batch Loss:     2.652218, Tokens per Sec:     5545, Lr: 0.000032\r\n",
      "2022-08-02 20:16:53,594 - INFO - joeynmt.training - Epoch  26: total training loss 2158.99\r\n",
      "2022-08-02 20:16:53,594 - INFO - joeynmt.training - EPOCH 27\r\n",
      "2022-08-02 20:17:07,058 - INFO - joeynmt.training - Epoch  27, Step:   476400, Batch Loss:     2.576868, Tokens per Sec:     5504, Lr: 0.000032\r\n",
      "2022-08-02 20:17:25,597 - INFO - joeynmt.training - Epoch  27, Step:   476500, Batch Loss:     2.514657, Tokens per Sec:     5523, Lr: 0.000032\r\n",
      "2022-08-02 20:17:44,009 - INFO - joeynmt.training - Epoch  27, Step:   476600, Batch Loss:     2.417922, Tokens per Sec:     5551, Lr: 0.000032\r\n",
      "2022-08-02 20:18:02,306 - INFO - joeynmt.training - Epoch  27, Step:   476700, Batch Loss:     2.713252, Tokens per Sec:     5293, Lr: 0.000032\r\n",
      "2022-08-02 20:18:20,580 - INFO - joeynmt.training - Epoch  27, Step:   476800, Batch Loss:     2.358624, Tokens per Sec:     5434, Lr: 0.000032\r\n",
      "2022-08-02 20:18:38,978 - INFO - joeynmt.training - Epoch  27, Step:   476900, Batch Loss:     2.437863, Tokens per Sec:     5540, Lr: 0.000032\r\n",
      "2022-08-02 20:18:57,247 - INFO - joeynmt.training - Epoch  27, Step:   477000, Batch Loss:     2.648877, Tokens per Sec:     5559, Lr: 0.000032\r\n",
      "2022-08-02 20:19:34,164 - INFO - joeynmt.helpers - delete models/deen_transformer/475000.ckpt\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:19:34,336 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> then to see how far you open\r\n",
      "2022-08-02 20:19:34,337 - INFO - joeynmt.training - Validation result (greedy) at epoch  27, step   477000: bleu:   2.15, loss: 45993.3945, ppl:  15.2105, duration: 37.0900s\r\n",
      "2022-08-02 20:19:52,757 - INFO - joeynmt.training - Epoch  27, Step:   477100, Batch Loss:     2.469630, Tokens per Sec:     5469, Lr: 0.000032\r\n",
      "2022-08-02 20:20:06,887 - INFO - joeynmt.training - Epoch  27: total training loss 2133.23\r\n",
      "2022-08-02 20:20:06,888 - INFO - joeynmt.training - EPOCH 28\r\n",
      "2022-08-02 20:20:11,062 - INFO - joeynmt.training - Epoch  28, Step:   477200, Batch Loss:     2.433412, Tokens per Sec:     5295, Lr: 0.000032\r\n",
      "2022-08-02 20:20:29,535 - INFO - joeynmt.training - Epoch  28, Step:   477300, Batch Loss:     2.626615, Tokens per Sec:     5492, Lr: 0.000032\r\n",
      "2022-08-02 20:20:47,816 - INFO - joeynmt.training - Epoch  28, Step:   477400, Batch Loss:     2.393007, Tokens per Sec:     5528, Lr: 0.000032\r\n",
      "2022-08-02 20:21:05,995 - INFO - joeynmt.training - Epoch  28, Step:   477500, Batch Loss:     2.542967, Tokens per Sec:     5438, Lr: 0.000032\r\n",
      "2022-08-02 20:21:24,239 - INFO - joeynmt.training - Epoch  28, Step:   477600, Batch Loss:     2.532298, Tokens per Sec:     5406, Lr: 0.000032\r\n",
      "2022-08-02 20:21:42,552 - INFO - joeynmt.training - Epoch  28, Step:   477700, Batch Loss:     2.652418, Tokens per Sec:     5442, Lr: 0.000032\r\n",
      "2022-08-02 20:22:00,962 - INFO - joeynmt.training - Epoch  28, Step:   477800, Batch Loss:     2.452301, Tokens per Sec:     5502, Lr: 0.000032\r\n",
      "2022-08-02 20:22:19,373 - INFO - joeynmt.training - Epoch  28, Step:   477900, Batch Loss:     2.594898, Tokens per Sec:     5411, Lr: 0.000032\r\n",
      "2022-08-02 20:22:37,775 - INFO - joeynmt.training - Epoch  28, Step:   478000, Batch Loss:     2.438576, Tokens per Sec:     5501, Lr: 0.000032\r\n",
      "2022-08-02 20:23:14,770 - INFO - joeynmt.helpers - delete models/deen_transformer/477000.ckpt\r\n",
      "2022-08-02 20:23:14,945 - INFO - joeynmt.helpers - delete /home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/477000.ckpt\r\n",
      "2022-08-02 20:23:14,946 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/477000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/models/deen_transformer/477000.ckpt')\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:23:14,949 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:23:14,950 - INFO - joeynmt.training - Validation result (greedy) at epoch  28, step   478000: bleu:   1.95, loss: 45967.3633, ppl:  15.1871, duration: 37.1745s\r\n",
      "2022-08-02 20:23:20,634 - INFO - joeynmt.training - Epoch  28: total training loss 2123.99\r\n",
      "2022-08-02 20:23:20,634 - INFO - joeynmt.training - EPOCH 29\r\n",
      "2022-08-02 20:23:33,568 - INFO - joeynmt.training - Epoch  29, Step:   478100, Batch Loss:     2.463759, Tokens per Sec:     5344, Lr: 0.000032\r\n",
      "2022-08-02 20:23:52,006 - INFO - joeynmt.training - Epoch  29, Step:   478200, Batch Loss:     2.378027, Tokens per Sec:     5435, Lr: 0.000032\r\n",
      "2022-08-02 20:24:10,293 - INFO - joeynmt.training - Epoch  29, Step:   478300, Batch Loss:     2.514734, Tokens per Sec:     5467, Lr: 0.000032\r\n",
      "2022-08-02 20:24:28,483 - INFO - joeynmt.training - Epoch  29, Step:   478400, Batch Loss:     2.456833, Tokens per Sec:     5380, Lr: 0.000032\r\n",
      "2022-08-02 20:24:46,999 - INFO - joeynmt.training - Epoch  29, Step:   478500, Batch Loss:     2.416175, Tokens per Sec:     5464, Lr: 0.000032\r\n",
      "2022-08-02 20:25:05,405 - INFO - joeynmt.training - Epoch  29, Step:   478600, Batch Loss:     2.509342, Tokens per Sec:     5506, Lr: 0.000032\r\n",
      "2022-08-02 20:25:23,785 - INFO - joeynmt.training - Epoch  29, Step:   478700, Batch Loss:     2.465547, Tokens per Sec:     5376, Lr: 0.000032\r\n",
      "2022-08-02 20:25:42,201 - INFO - joeynmt.training - Epoch  29, Step:   478800, Batch Loss:     2.579797, Tokens per Sec:     5494, Lr: 0.000032\r\n",
      "2022-08-02 20:25:57,872 - INFO - joeynmt.training - Epoch  29: total training loss 2115.45\r\n",
      "2022-08-02 20:25:57,873 - INFO - joeynmt.training - EPOCH 30\r\n",
      "2022-08-02 20:26:00,568 - INFO - joeynmt.training - Epoch  30, Step:   478900, Batch Loss:     2.408360, Tokens per Sec:     4913, Lr: 0.000032\r\n",
      "2022-08-02 20:26:18,947 - INFO - joeynmt.training - Epoch  30, Step:   479000, Batch Loss:     2.471340, Tokens per Sec:     5213, Lr: 0.000032\r\n",
      "2022-08-02 20:26:54,346 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:26:54,346 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:26:54,346 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:26:54,347 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:26:54,348 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:26:54,348 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> then to see how far her <unk> opened\r\n",
      "2022-08-02 20:26:54,348 - INFO - joeynmt.training - Validation result (greedy) at epoch  30, step   479000: bleu:   1.87, loss: 46046.3984, ppl:  15.2583, duration: 35.3999s\r\n",
      "2022-08-02 20:27:12,667 - INFO - joeynmt.training - Epoch  30, Step:   479100, Batch Loss:     2.489435, Tokens per Sec:     5341, Lr: 0.000032\r\n",
      "2022-08-02 20:27:30,826 - INFO - joeynmt.training - Epoch  30, Step:   479200, Batch Loss:     2.585370, Tokens per Sec:     5455, Lr: 0.000032\r\n",
      "2022-08-02 20:27:49,485 - INFO - joeynmt.training - Epoch  30, Step:   479300, Batch Loss:     2.484009, Tokens per Sec:     5596, Lr: 0.000032\r\n",
      "2022-08-02 20:28:07,815 - INFO - joeynmt.training - Epoch  30, Step:   479400, Batch Loss:     2.412787, Tokens per Sec:     5506, Lr: 0.000032\r\n",
      "2022-08-02 20:28:26,045 - INFO - joeynmt.training - Epoch  30, Step:   479500, Batch Loss:     2.268995, Tokens per Sec:     5464, Lr: 0.000032\r\n",
      "2022-08-02 20:28:44,548 - INFO - joeynmt.training - Epoch  30, Step:   479600, Batch Loss:     2.461480, Tokens per Sec:     5478, Lr: 0.000032\r\n",
      "2022-08-02 20:29:02,902 - INFO - joeynmt.training - Epoch  30, Step:   479700, Batch Loss:     2.279246, Tokens per Sec:     5565, Lr: 0.000032\r\n",
      "2022-08-02 20:29:10,525 - INFO - joeynmt.training - Epoch  30: total training loss 2098.92\r\n",
      "2022-08-02 20:29:10,526 - INFO - joeynmt.training - EPOCH 31\r\n",
      "2022-08-02 20:29:21,272 - INFO - joeynmt.training - Epoch  31, Step:   479800, Batch Loss:     2.392102, Tokens per Sec:     5394, Lr: 0.000032\r\n",
      "2022-08-02 20:29:39,764 - INFO - joeynmt.training - Epoch  31, Step:   479900, Batch Loss:     2.327806, Tokens per Sec:     5335, Lr: 0.000032\r\n",
      "2022-08-02 20:29:58,104 - INFO - joeynmt.training - Epoch  31, Step:   480000, Batch Loss:     2.425342, Tokens per Sec:     5647, Lr: 0.000032\r\n",
      "2022-08-02 20:30:33,437 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> at the same time\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:30:33,438 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> was about the <unk> called <unk> ping\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:30:33,439 - INFO - joeynmt.training - Validation result (greedy) at epoch  31, step   480000: bleu:   2.37, loss: 46195.8555, ppl:  15.3938, duration: 35.3345s\r\n",
      "2022-08-02 20:30:51,982 - INFO - joeynmt.training - Epoch  31, Step:   480100, Batch Loss:     2.523062, Tokens per Sec:     5579, Lr: 0.000032\r\n",
      "2022-08-02 20:31:10,386 - INFO - joeynmt.training - Epoch  31, Step:   480200, Batch Loss:     2.361769, Tokens per Sec:     5420, Lr: 0.000032\r\n",
      "2022-08-02 20:31:28,687 - INFO - joeynmt.training - Epoch  31, Step:   480300, Batch Loss:     2.207593, Tokens per Sec:     5480, Lr: 0.000032\r\n",
      "2022-08-02 20:31:47,140 - INFO - joeynmt.training - Epoch  31, Step:   480400, Batch Loss:     2.497176, Tokens per Sec:     5329, Lr: 0.000032\r\n",
      "2022-08-02 20:32:05,472 - INFO - joeynmt.training - Epoch  31, Step:   480500, Batch Loss:     2.490417, Tokens per Sec:     5475, Lr: 0.000032\r\n",
      "2022-08-02 20:32:22,775 - INFO - joeynmt.training - Epoch  31: total training loss 2075.47\r\n",
      "2022-08-02 20:32:22,776 - INFO - joeynmt.training - EPOCH 32\r\n",
      "2022-08-02 20:32:23,807 - INFO - joeynmt.training - Epoch  32, Step:   480600, Batch Loss:     2.491038, Tokens per Sec:     4603, Lr: 0.000032\r\n",
      "2022-08-02 20:32:42,203 - INFO - joeynmt.training - Epoch  32, Step:   480700, Batch Loss:     2.500153, Tokens per Sec:     5571, Lr: 0.000032\r\n",
      "2022-08-02 20:33:00,868 - INFO - joeynmt.training - Epoch  32, Step:   480800, Batch Loss:     2.379697, Tokens per Sec:     5449, Lr: 0.000032\r\n",
      "2022-08-02 20:33:19,412 - INFO - joeynmt.training - Epoch  32, Step:   480900, Batch Loss:     2.355036, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:33:37,789 - INFO - joeynmt.training - Epoch  32, Step:   481000, Batch Loss:     2.618155, Tokens per Sec:     5438, Lr: 0.000032\r\n",
      "2022-08-02 20:34:13,406 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:34:13,406 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:34:13,406 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:34:13,406 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:34:13,406 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> at the same time\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far her <unk> opened\r\n",
      "2022-08-02 20:34:13,407 - INFO - joeynmt.training - Validation result (greedy) at epoch  32, step   481000: bleu:   2.07, loss: 46154.1641, ppl:  15.3559, duration: 35.6184s\r\n",
      "2022-08-02 20:34:31,778 - INFO - joeynmt.training - Epoch  32, Step:   481100, Batch Loss:     2.538485, Tokens per Sec:     5519, Lr: 0.000032\r\n",
      "2022-08-02 20:34:50,271 - INFO - joeynmt.training - Epoch  32, Step:   481200, Batch Loss:     2.512504, Tokens per Sec:     5465, Lr: 0.000032\r\n",
      "2022-08-02 20:35:08,643 - INFO - joeynmt.training - Epoch  32, Step:   481300, Batch Loss:     2.484198, Tokens per Sec:     5445, Lr: 0.000032\r\n",
      "2022-08-02 20:35:27,058 - INFO - joeynmt.training - Epoch  32, Step:   481400, Batch Loss:     2.365307, Tokens per Sec:     5513, Lr: 0.000032\r\n",
      "2022-08-02 20:35:34,336 - INFO - joeynmt.training - Epoch  32: total training loss 2042.78\r\n",
      "2022-08-02 20:35:34,336 - INFO - joeynmt.training - EPOCH 33\r\n",
      "2022-08-02 20:35:45,577 - INFO - joeynmt.training - Epoch  33, Step:   481500, Batch Loss:     2.333414, Tokens per Sec:     5342, Lr: 0.000032\r\n",
      "2022-08-02 20:36:03,939 - INFO - joeynmt.training - Epoch  33, Step:   481600, Batch Loss:     2.312297, Tokens per Sec:     5397, Lr: 0.000032\r\n",
      "2022-08-02 20:36:22,378 - INFO - joeynmt.training - Epoch  33, Step:   481700, Batch Loss:     2.163956, Tokens per Sec:     5400, Lr: 0.000032\r\n",
      "2022-08-02 20:36:40,756 - INFO - joeynmt.training - Epoch  33, Step:   481800, Batch Loss:     2.427604, Tokens per Sec:     5519, Lr: 0.000032\r\n",
      "2022-08-02 20:36:59,047 - INFO - joeynmt.training - Epoch  33, Step:   481900, Batch Loss:     2.429536, Tokens per Sec:     5445, Lr: 0.000032\r\n",
      "2022-08-02 20:37:17,517 - INFO - joeynmt.training - Epoch  33, Step:   482000, Batch Loss:     2.635306, Tokens per Sec:     5541, Lr: 0.000032\r\n",
      "2022-08-02 20:37:52,824 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:37:52,824 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:37:52,824 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:37:52,824 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:37:52,824 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:37:52,825 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:37:52,826 - INFO - joeynmt.training - Validation result (greedy) at epoch  33, step   482000: bleu:   2.46, loss: 46199.0000, ppl:  15.3967, duration: 35.3081s\r\n",
      "2022-08-02 20:38:11,173 - INFO - joeynmt.training - Epoch  33, Step:   482100, Batch Loss:     2.506704, Tokens per Sec:     5550, Lr: 0.000032\r\n",
      "2022-08-02 20:38:29,741 - INFO - joeynmt.training - Epoch  33, Step:   482200, Batch Loss:     2.376115, Tokens per Sec:     5423, Lr: 0.000032\r\n",
      "2022-08-02 20:38:46,735 - INFO - joeynmt.training - Epoch  33: total training loss 2048.81\r\n",
      "2022-08-02 20:38:46,735 - INFO - joeynmt.training - EPOCH 34\r\n",
      "2022-08-02 20:38:47,982 - INFO - joeynmt.training - Epoch  34, Step:   482300, Batch Loss:     2.367724, Tokens per Sec:     4338, Lr: 0.000032\r\n",
      "2022-08-02 20:39:06,381 - INFO - joeynmt.training - Epoch  34, Step:   482400, Batch Loss:     2.400790, Tokens per Sec:     5509, Lr: 0.000032\r\n",
      "2022-08-02 20:39:24,854 - INFO - joeynmt.training - Epoch  34, Step:   482500, Batch Loss:     2.575421, Tokens per Sec:     5430, Lr: 0.000032\r\n",
      "2022-08-02 20:39:43,161 - INFO - joeynmt.training - Epoch  34, Step:   482600, Batch Loss:     2.398110, Tokens per Sec:     5564, Lr: 0.000032\r\n",
      "2022-08-02 20:40:01,593 - INFO - joeynmt.training - Epoch  34, Step:   482700, Batch Loss:     2.353582, Tokens per Sec:     5546, Lr: 0.000032\r\n",
      "2022-08-02 20:40:19,871 - INFO - joeynmt.training - Epoch  34, Step:   482800, Batch Loss:     2.455136, Tokens per Sec:     5448, Lr: 0.000032\r\n",
      "2022-08-02 20:40:38,158 - INFO - joeynmt.training - Epoch  34, Step:   482900, Batch Loss:     2.288275, Tokens per Sec:     5336, Lr: 0.000032\r\n",
      "2022-08-02 20:40:56,544 - INFO - joeynmt.training - Epoch  34, Step:   483000, Batch Loss:     2.299103, Tokens per Sec:     5477, Lr: 0.000032\r\n",
      "2022-08-02 20:41:32,266 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:41:32,267 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:41:32,268 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:41:32,268 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:41:32,268 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:41:32,268 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> a <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:41:32,268 - INFO - joeynmt.training - Validation result (greedy) at epoch  34, step   483000: bleu:   2.31, loss: 46235.6992, ppl:  15.4302, duration: 35.7232s\r\n",
      "2022-08-02 20:41:50,521 - INFO - joeynmt.training - Epoch  34, Step:   483100, Batch Loss:     2.206094, Tokens per Sec:     5485, Lr: 0.000032\r\n",
      "2022-08-02 20:41:58,822 - INFO - joeynmt.training - Epoch  34: total training loss 2029.14\r\n",
      "2022-08-02 20:41:58,822 - INFO - joeynmt.training - EPOCH 35\r\n",
      "2022-08-02 20:42:08,797 - INFO - joeynmt.training - Epoch  35, Step:   483200, Batch Loss:     2.268821, Tokens per Sec:     5442, Lr: 0.000032\r\n",
      "2022-08-02 20:42:26,946 - INFO - joeynmt.training - Epoch  35, Step:   483300, Batch Loss:     2.250931, Tokens per Sec:     5473, Lr: 0.000032\r\n",
      "2022-08-02 20:42:45,309 - INFO - joeynmt.training - Epoch  35, Step:   483400, Batch Loss:     2.217171, Tokens per Sec:     5396, Lr: 0.000032\r\n",
      "2022-08-02 20:43:03,581 - INFO - joeynmt.training - Epoch  35, Step:   483500, Batch Loss:     2.188474, Tokens per Sec:     5505, Lr: 0.000032\r\n",
      "2022-08-02 20:43:22,000 - INFO - joeynmt.training - Epoch  35, Step:   483600, Batch Loss:     2.176793, Tokens per Sec:     5452, Lr: 0.000032\r\n",
      "2022-08-02 20:43:40,233 - INFO - joeynmt.training - Epoch  35, Step:   483700, Batch Loss:     2.516809, Tokens per Sec:     5525, Lr: 0.000032\r\n",
      "2022-08-02 20:43:58,521 - INFO - joeynmt.training - Epoch  35, Step:   483800, Batch Loss:     2.357411, Tokens per Sec:     5471, Lr: 0.000032\r\n",
      "2022-08-02 20:44:16,886 - INFO - joeynmt.training - Epoch  35, Step:   483900, Batch Loss:     2.431102, Tokens per Sec:     5509, Lr: 0.000032\r\n",
      "2022-08-02 20:44:35,241 - INFO - joeynmt.training - Epoch  35, Step:   484000, Batch Loss:     2.654027, Tokens per Sec:     5420, Lr: 0.000032\r\n",
      "2022-08-02 20:45:10,513 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk>\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> at once\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:45:10,514 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:45:10,515 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:45:10,515 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:45:10,515 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> then to see how far you open\r\n",
      "2022-08-02 20:45:10,515 - INFO - joeynmt.training - Validation result (greedy) at epoch  35, step   484000: bleu:   2.02, loss: 46191.7695, ppl:  15.3901, duration: 35.2733s\r\n",
      "2022-08-02 20:45:10,523 - INFO - joeynmt.training - Epoch  35: total training loss 2019.30\r\n",
      "2022-08-02 20:45:10,524 - INFO - joeynmt.training - EPOCH 36\r\n",
      "2022-08-02 20:45:28,992 - INFO - joeynmt.training - Epoch  36, Step:   484100, Batch Loss:     2.273173, Tokens per Sec:     5303, Lr: 0.000032\r\n",
      "2022-08-02 20:45:47,472 - INFO - joeynmt.training - Epoch  36, Step:   484200, Batch Loss:     2.504184, Tokens per Sec:     5438, Lr: 0.000032\r\n",
      "2022-08-02 20:46:05,805 - INFO - joeynmt.training - Epoch  36, Step:   484300, Batch Loss:     2.353251, Tokens per Sec:     5502, Lr: 0.000032\r\n",
      "2022-08-02 20:46:24,397 - INFO - joeynmt.training - Epoch  36, Step:   484400, Batch Loss:     2.291454, Tokens per Sec:     5553, Lr: 0.000032\r\n",
      "2022-08-02 20:46:42,734 - INFO - joeynmt.training - Epoch  36, Step:   484500, Batch Loss:     2.343680, Tokens per Sec:     5567, Lr: 0.000032\r\n",
      "2022-08-02 20:47:01,020 - INFO - joeynmt.training - Epoch  36, Step:   484600, Batch Loss:     2.377499, Tokens per Sec:     5430, Lr: 0.000032\r\n",
      "2022-08-02 20:47:19,164 - INFO - joeynmt.training - Epoch  36, Step:   484700, Batch Loss:     2.439440, Tokens per Sec:     5390, Lr: 0.000032\r\n",
      "2022-08-02 20:47:37,590 - INFO - joeynmt.training - Epoch  36, Step:   484800, Batch Loss:     2.440888, Tokens per Sec:     5453, Lr: 0.000032\r\n",
      "2022-08-02 20:47:47,117 - INFO - joeynmt.training - Epoch  36: total training loss 1998.94\r\n",
      "2022-08-02 20:47:47,117 - INFO - joeynmt.training - EPOCH 37\r\n",
      "2022-08-02 20:47:56,128 - INFO - joeynmt.training - Epoch  37, Step:   484900, Batch Loss:     2.367953, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:48:14,646 - INFO - joeynmt.training - Epoch  37, Step:   485000, Batch Loss:     2.188703, Tokens per Sec:     5599, Lr: 0.000032\r\n",
      "2022-08-02 20:48:50,225 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:48:50,225 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> <unk> the <unk>\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we were doing it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:48:50,226 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:48:50,227 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:48:50,227 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:48:50,227 - INFO - joeynmt.training - Validation result (greedy) at epoch  37, step   485000: bleu:   1.92, loss: 46519.2148, ppl:  15.6913, duration: 35.5801s\r\n",
      "2022-08-02 20:49:08,708 - INFO - joeynmt.training - Epoch  37, Step:   485100, Batch Loss:     2.245250, Tokens per Sec:     5270, Lr: 0.000032\r\n",
      "2022-08-02 20:49:27,149 - INFO - joeynmt.training - Epoch  37, Step:   485200, Batch Loss:     2.404807, Tokens per Sec:     5462, Lr: 0.000032\r\n",
      "2022-08-02 20:49:45,556 - INFO - joeynmt.training - Epoch  37, Step:   485300, Batch Loss:     2.286182, Tokens per Sec:     5447, Lr: 0.000032\r\n",
      "2022-08-02 20:50:04,155 - INFO - joeynmt.training - Epoch  37, Step:   485400, Batch Loss:     2.287818, Tokens per Sec:     5403, Lr: 0.000032\r\n",
      "2022-08-02 20:50:22,372 - INFO - joeynmt.training - Epoch  37, Step:   485500, Batch Loss:     2.232112, Tokens per Sec:     5473, Lr: 0.000032\r\n",
      "2022-08-02 20:50:40,882 - INFO - joeynmt.training - Epoch  37, Step:   485600, Batch Loss:     2.430543, Tokens per Sec:     5483, Lr: 0.000032\r\n",
      "2022-08-02 20:50:59,056 - INFO - joeynmt.training - Epoch  37, Step:   485700, Batch Loss:     2.353800, Tokens per Sec:     5390, Lr: 0.000032\r\n",
      "2022-08-02 20:50:59,789 - INFO - joeynmt.training - Epoch  37: total training loss 1984.44\r\n",
      "2022-08-02 20:50:59,790 - INFO - joeynmt.training - EPOCH 38\r\n",
      "2022-08-02 20:51:17,449 - INFO - joeynmt.training - Epoch  38, Step:   485800, Batch Loss:     2.239099, Tokens per Sec:     5402, Lr: 0.000032\r\n",
      "2022-08-02 20:51:35,830 - INFO - joeynmt.training - Epoch  38, Step:   485900, Batch Loss:     2.385672, Tokens per Sec:     5304, Lr: 0.000032\r\n",
      "2022-08-02 20:51:54,168 - INFO - joeynmt.training - Epoch  38, Step:   486000, Batch Loss:     2.331247, Tokens per Sec:     5564, Lr: 0.000032\r\n",
      "2022-08-02 20:52:30,933 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the two <unk> <unk> at the same time\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:52:30,934 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:52:30,935 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:52:30,935 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:52:30,935 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open their <unk>\r\n",
      "2022-08-02 20:52:30,935 - INFO - joeynmt.training - Validation result (greedy) at epoch  38, step   486000: bleu:   2.43, loss: 46643.6758, ppl:  15.8073, duration: 36.7659s\r\n",
      "2022-08-02 20:52:49,211 - INFO - joeynmt.training - Epoch  38, Step:   486100, Batch Loss:     2.358304, Tokens per Sec:     5455, Lr: 0.000032\r\n",
      "2022-08-02 20:53:07,722 - INFO - joeynmt.training - Epoch  38, Step:   486200, Batch Loss:     2.375975, Tokens per Sec:     5401, Lr: 0.000032\r\n",
      "2022-08-02 20:53:26,129 - INFO - joeynmt.training - Epoch  38, Step:   486300, Batch Loss:     2.344514, Tokens per Sec:     5623, Lr: 0.000032\r\n",
      "2022-08-02 20:53:44,702 - INFO - joeynmt.training - Epoch  38, Step:   486400, Batch Loss:     2.150945, Tokens per Sec:     5325, Lr: 0.000032\r\n",
      "2022-08-02 20:54:03,028 - INFO - joeynmt.training - Epoch  38, Step:   486500, Batch Loss:     2.405016, Tokens per Sec:     5406, Lr: 0.000032\r\n",
      "2022-08-02 20:54:13,508 - INFO - joeynmt.training - Epoch  38: total training loss 1970.34\r\n",
      "2022-08-02 20:54:13,508 - INFO - joeynmt.training - EPOCH 39\r\n",
      "2022-08-02 20:54:21,560 - INFO - joeynmt.training - Epoch  39, Step:   486600, Batch Loss:     2.300603, Tokens per Sec:     5482, Lr: 0.000032\r\n",
      "2022-08-02 20:54:39,834 - INFO - joeynmt.training - Epoch  39, Step:   486700, Batch Loss:     2.232083, Tokens per Sec:     5428, Lr: 0.000032\r\n",
      "2022-08-02 20:54:58,426 - INFO - joeynmt.training - Epoch  39, Step:   486800, Batch Loss:     2.275684, Tokens per Sec:     5370, Lr: 0.000032\r\n",
      "2022-08-02 20:55:16,730 - INFO - joeynmt.training - Epoch  39, Step:   486900, Batch Loss:     2.473503, Tokens per Sec:     5616, Lr: 0.000032\r\n",
      "2022-08-02 20:55:35,250 - INFO - joeynmt.training - Epoch  39, Step:   487000, Batch Loss:     2.283029, Tokens per Sec:     5479, Lr: 0.000032\r\n",
      "2022-08-02 20:56:10,531 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the two <unk> at once\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 20:56:10,532 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:56:10,533 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:56:10,533 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:56:10,533 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open\r\n",
      "2022-08-02 20:56:10,533 - INFO - joeynmt.training - Validation result (greedy) at epoch  39, step   487000: bleu:   2.38, loss: 46632.7539, ppl:  15.7971, duration: 35.2827s\r\n",
      "2022-08-02 20:56:28,651 - INFO - joeynmt.training - Epoch  39, Step:   487100, Batch Loss:     2.494102, Tokens per Sec:     5413, Lr: 0.000032\r\n",
      "2022-08-02 20:56:46,989 - INFO - joeynmt.training - Epoch  39, Step:   487200, Batch Loss:     2.186885, Tokens per Sec:     5480, Lr: 0.000032\r\n",
      "2022-08-02 20:57:05,356 - INFO - joeynmt.training - Epoch  39, Step:   487300, Batch Loss:     2.372311, Tokens per Sec:     5459, Lr: 0.000032\r\n",
      "2022-08-02 20:57:23,637 - INFO - joeynmt.training - Epoch  39, Step:   487400, Batch Loss:     2.326195, Tokens per Sec:     5386, Lr: 0.000032\r\n",
      "2022-08-02 20:57:25,717 - INFO - joeynmt.training - Epoch  39: total training loss 1962.91\r\n",
      "2022-08-02 20:57:25,718 - INFO - joeynmt.training - EPOCH 40\r\n",
      "2022-08-02 20:57:42,038 - INFO - joeynmt.training - Epoch  40, Step:   487500, Batch Loss:     2.114459, Tokens per Sec:     5369, Lr: 0.000032\r\n",
      "2022-08-02 20:58:00,211 - INFO - joeynmt.training - Epoch  40, Step:   487600, Batch Loss:     2.212930, Tokens per Sec:     5348, Lr: 0.000032\r\n",
      "2022-08-02 20:58:18,436 - INFO - joeynmt.training - Epoch  40, Step:   487700, Batch Loss:     2.358433, Tokens per Sec:     5528, Lr: 0.000032\r\n",
      "2022-08-02 20:58:36,726 - INFO - joeynmt.training - Epoch  40, Step:   487800, Batch Loss:     2.277170, Tokens per Sec:     5515, Lr: 0.000032\r\n",
      "2022-08-02 20:58:55,207 - INFO - joeynmt.training - Epoch  40, Step:   487900, Batch Loss:     2.126087, Tokens per Sec:     5458, Lr: 0.000032\r\n",
      "2022-08-02 20:59:13,604 - INFO - joeynmt.training - Epoch  40, Step:   488000, Batch Loss:     2.228793, Tokens per Sec:     5425, Lr: 0.000032\r\n",
      "2022-08-02 20:59:48,987 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it was called the <unk> ga\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 20:59:48,988 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 20:59:48,989 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 20:59:48,989 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open their <unk>\r\n",
      "2022-08-02 20:59:48,989 - INFO - joeynmt.training - Validation result (greedy) at epoch  40, step   488000: bleu:   1.87, loss: 46623.1484, ppl:  15.7881, duration: 35.3846s\r\n",
      "2022-08-02 21:00:07,412 - INFO - joeynmt.training - Epoch  40, Step:   488100, Batch Loss:     2.203899, Tokens per Sec:     5392, Lr: 0.000032\r\n",
      "2022-08-02 21:00:25,944 - INFO - joeynmt.training - Epoch  40, Step:   488200, Batch Loss:     2.303455, Tokens per Sec:     5431, Lr: 0.000032\r\n",
      "2022-08-02 21:00:38,637 - INFO - joeynmt.training - Epoch  40: total training loss 1956.83\r\n",
      "2022-08-02 21:00:38,637 - INFO - joeynmt.training - EPOCH 41\r\n",
      "2022-08-02 21:00:44,103 - INFO - joeynmt.training - Epoch  41, Step:   488300, Batch Loss:     2.344612, Tokens per Sec:     5208, Lr: 0.000032\r\n",
      "2022-08-02 21:01:02,731 - INFO - joeynmt.training - Epoch  41, Step:   488400, Batch Loss:     2.272541, Tokens per Sec:     5540, Lr: 0.000032\r\n",
      "2022-08-02 21:01:21,158 - INFO - joeynmt.training - Epoch  41, Step:   488500, Batch Loss:     2.341237, Tokens per Sec:     5481, Lr: 0.000032\r\n",
      "2022-08-02 21:01:39,673 - INFO - joeynmt.training - Epoch  41, Step:   488600, Batch Loss:     2.419485, Tokens per Sec:     5487, Lr: 0.000032\r\n",
      "2022-08-02 21:01:58,082 - INFO - joeynmt.training - Epoch  41, Step:   488700, Batch Loss:     2.375217, Tokens per Sec:     5412, Lr: 0.000032\r\n",
      "2022-08-02 21:02:16,360 - INFO - joeynmt.training - Epoch  41, Step:   488800, Batch Loss:     2.401582, Tokens per Sec:     5391, Lr: 0.000032\r\n",
      "2022-08-02 21:02:34,779 - INFO - joeynmt.training - Epoch  41, Step:   488900, Batch Loss:     2.131480, Tokens per Sec:     5476, Lr: 0.000032\r\n",
      "2022-08-02 21:02:53,777 - INFO - joeynmt.training - Epoch  41, Step:   489000, Batch Loss:     2.333513, Tokens per Sec:     5387, Lr: 0.000032\r\n",
      "2022-08-02 21:03:29,156 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 21:03:29,156 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 21:03:29,156 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at once\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> were working it to <unk> called the <unk> ga\r\n",
      "2022-08-02 21:03:29,157 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 21:03:29,158 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 21:03:29,158 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 21:03:29,158 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open their <unk>\r\n",
      "2022-08-02 21:03:29,158 - INFO - joeynmt.training - Validation result (greedy) at epoch  41, step   489000: bleu:   2.58, loss: 46659.8281, ppl:  15.8224, duration: 35.3801s\r\n",
      "2022-08-02 21:03:47,697 - INFO - joeynmt.training - Epoch  41, Step:   489100, Batch Loss:     2.333067, Tokens per Sec:     5386, Lr: 0.000032\r\n",
      "2022-08-02 21:03:51,322 - INFO - joeynmt.training - Epoch  41: total training loss 1924.10\r\n",
      "2022-08-02 21:03:51,322 - INFO - joeynmt.training - EPOCH 42\r\n",
      "2022-08-02 21:04:06,069 - INFO - joeynmt.training - Epoch  42, Step:   489200, Batch Loss:     2.423508, Tokens per Sec:     5388, Lr: 0.000032\r\n",
      "2022-08-02 21:04:24,434 - INFO - joeynmt.training - Epoch  42, Step:   489300, Batch Loss:     2.189701, Tokens per Sec:     5393, Lr: 0.000032\r\n",
      "2022-08-02 21:04:42,804 - INFO - joeynmt.training - Epoch  42, Step:   489400, Batch Loss:     2.235437, Tokens per Sec:     5609, Lr: 0.000032\r\n",
      "2022-08-02 21:05:01,246 - INFO - joeynmt.training - Epoch  42, Step:   489500, Batch Loss:     2.218063, Tokens per Sec:     5375, Lr: 0.000032\r\n",
      "2022-08-02 21:05:19,615 - INFO - joeynmt.training - Epoch  42, Step:   489600, Batch Loss:     2.168681, Tokens per Sec:     5427, Lr: 0.000032\r\n",
      "2022-08-02 21:05:38,028 - INFO - joeynmt.training - Epoch  42, Step:   489700, Batch Loss:     2.330766, Tokens per Sec:     5424, Lr: 0.000032\r\n",
      "2022-08-02 21:05:56,759 - INFO - joeynmt.training - Epoch  42, Step:   489800, Batch Loss:     2.255663, Tokens per Sec:     5411, Lr: 0.000032\r\n",
      "2022-08-02 21:06:17,451 - INFO - joeynmt.training - Epoch  42, Step:   489900, Batch Loss:     2.105045, Tokens per Sec:     4780, Lr: 0.000032\r\n",
      "2022-08-02 21:06:31,103 - INFO - joeynmt.training - Epoch  42: total training loss 1925.89\r\n",
      "2022-08-02 21:06:31,103 - INFO - joeynmt.training - EPOCH 43\r\n",
      "2022-08-02 21:06:35,701 - INFO - joeynmt.training - Epoch  43, Step:   490000, Batch Loss:     2.300150, Tokens per Sec:     5015, Lr: 0.000032\r\n",
      "2022-08-02 21:07:11,488 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 21:07:11,488 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 21:07:11,488 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 21:07:11,488 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 21:07:11,488 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> over the <unk> one of the <unk> <unk> <unk> at the same time\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it to <unk> called the <unk> ga\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 21:07:11,489 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 21:07:11,490 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 21:07:11,490 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see the <unk> <unk> as far as you open their <unk>\r\n",
      "2022-08-02 21:07:11,490 - INFO - joeynmt.training - Validation result (greedy) at epoch  43, step   490000: bleu:   2.13, loss: 47137.7344, ppl:  16.2763, duration: 35.7881s\r\n",
      "2022-08-02 21:07:29,877 - INFO - joeynmt.training - Epoch  43, Step:   490100, Batch Loss:     2.236290, Tokens per Sec:     5384, Lr: 0.000032\r\n",
      "2022-08-02 21:07:48,039 - INFO - joeynmt.training - Epoch  43, Step:   490200, Batch Loss:     2.172787, Tokens per Sec:     5506, Lr: 0.000032\r\n",
      "2022-08-02 21:08:06,549 - INFO - joeynmt.training - Epoch  43, Step:   490300, Batch Loss:     2.436768, Tokens per Sec:     5466, Lr: 0.000032\r\n",
      "2022-08-02 21:08:24,757 - INFO - joeynmt.training - Epoch  43, Step:   490400, Batch Loss:     2.344624, Tokens per Sec:     5425, Lr: 0.000032\r\n",
      "2022-08-02 21:08:43,211 - INFO - joeynmt.training - Epoch  43, Step:   490500, Batch Loss:     2.344672, Tokens per Sec:     5383, Lr: 0.000032\r\n",
      "2022-08-02 21:09:01,529 - INFO - joeynmt.training - Epoch  43, Step:   490600, Batch Loss:     2.153574, Tokens per Sec:     5525, Lr: 0.000032\r\n",
      "2022-08-02 21:09:19,795 - INFO - joeynmt.training - Epoch  43, Step:   490700, Batch Loss:     2.251388, Tokens per Sec:     5317, Lr: 0.000032\r\n",
      "2022-08-02 21:09:38,140 - INFO - joeynmt.training - Epoch  43, Step:   490800, Batch Loss:     2.380972, Tokens per Sec:     5477, Lr: 0.000032\r\n",
      "2022-08-02 21:09:44,445 - INFO - joeynmt.training - Epoch  43: total training loss 1917.09\r\n",
      "2022-08-02 21:09:44,446 - INFO - joeynmt.training - EPOCH 44\r\n",
      "2022-08-02 21:09:57,350 - INFO - joeynmt.training - Epoch  44, Step:   490900, Batch Loss:     2.241188, Tokens per Sec:     5042, Lr: 0.000032\r\n",
      "2022-08-02 21:10:15,721 - INFO - joeynmt.training - Epoch  44, Step:   491000, Batch Loss:     2.237471, Tokens per Sec:     5317, Lr: 0.000032\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> <unk> at once\r\n",
      "2022-08-02 21:10:51,403 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> we went to work it was called the <unk> sodium\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see how far you open their <unk>\r\n",
      "2022-08-02 21:10:51,404 - INFO - joeynmt.training - Validation result (greedy) at epoch  44, step   491000: bleu:   2.25, loss: 47044.6211, ppl:  16.1869, duration: 35.6824s\r\n",
      "2022-08-02 21:11:09,754 - INFO - joeynmt.training - Epoch  44, Step:   491100, Batch Loss:     2.128377, Tokens per Sec:     5550, Lr: 0.000032\r\n",
      "2022-08-02 21:11:28,216 - INFO - joeynmt.training - Epoch  44, Step:   491200, Batch Loss:     2.230134, Tokens per Sec:     5350, Lr: 0.000032\r\n",
      "2022-08-02 21:11:46,324 - INFO - joeynmt.training - Epoch  44, Step:   491300, Batch Loss:     2.149148, Tokens per Sec:     5539, Lr: 0.000032\r\n",
      "2022-08-02 21:12:04,826 - INFO - joeynmt.training - Epoch  44, Step:   491400, Batch Loss:     2.165796, Tokens per Sec:     5414, Lr: 0.000032\r\n",
      "2022-08-02 21:12:22,918 - INFO - joeynmt.training - Epoch  44, Step:   491500, Batch Loss:     2.178667, Tokens per Sec:     5487, Lr: 0.000032\r\n",
      "2022-08-02 21:12:41,539 - INFO - joeynmt.training - Epoch  44, Step:   491600, Batch Loss:     2.234104, Tokens per Sec:     5603, Lr: 0.000032\r\n",
      "2022-08-02 21:12:57,756 - INFO - joeynmt.training - Epoch  44: total training loss 1892.31\r\n",
      "2022-08-02 21:12:57,757 - INFO - joeynmt.training - EPOCH 45\r\n",
      "2022-08-02 21:12:59,900 - INFO - joeynmt.training - Epoch  45, Step:   491700, Batch Loss:     2.184708, Tokens per Sec:     5089, Lr: 0.000032\r\n",
      "2022-08-02 21:13:18,140 - INFO - joeynmt.training - Epoch  45, Step:   491800, Batch Loss:     1.994850, Tokens per Sec:     5381, Lr: 0.000032\r\n",
      "2022-08-02 21:13:36,405 - INFO - joeynmt.training - Epoch  45, Step:   491900, Batch Loss:     2.117728, Tokens per Sec:     5343, Lr: 0.000032\r\n",
      "2022-08-02 21:13:54,578 - INFO - joeynmt.training - Epoch  45, Step:   492000, Batch Loss:     2.076687, Tokens per Sec:     5485, Lr: 0.000032\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - Example #0\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tSource:     Mich interessierten nicht seine politischen Aspekte sondern seine Formen die Farben die Gestaltung die Komposition\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tReference:  I refused to discuss its politics I studied its shapes the colors the forms composition\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> his <unk> <unk> <unk> his <unk> <unk> the <unk> <unk> the <unk> <unk>\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - Example #1\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tSource:     Meine Abschlussarbeit schrieb ich über das Gesicht einer Frau die zwei Schwänze gleichzeitig lutscht\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tReference:  For my undergraduate thesis I wrote about what happens to a womans face when she sucks two cocks at the same time\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> <unk> <unk> <unk> about the <unk> one of the two <unk> at once\r\n",
      "2022-08-02 21:14:29,806 - INFO - joeynmt.training - Example #2\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tSource:     In meiner Doktorarbeit ging es um das sogenannte Gaping\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tReference:  For my PhD I wrote about gaping\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tHypothesis: <unk> <unk> <unk> <unk> we were working it to <unk> called the <unk> ga\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - Example #3\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tSource:     Das ist wenn eine Gruppe von Männern einer Frau in den Arsch fickt um dann zu sehen wie weit sich ihr Arschloch geöffnet hat\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tReference:  Gaping in case you dont know is when a group of men fuck a woman in the asshole and then tape it open to see how wide its gotten from all the fucking\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - \tHypothesis: <unk> if a <unk> of <unk> <unk> in the <unk> to see the <unk> <unk> <unk> it out of the window\r\n",
      "2022-08-02 21:14:29,807 - INFO - joeynmt.training - Validation result (greedy) at epoch  45, step   492000: bleu:   2.18, loss: 47001.4609, ppl:  16.1456, duration: 35.2287s\r\n",
      "2022-08-02 21:14:48,228 - INFO - joeynmt.training - Epoch  45, Step:   492100, Batch Loss:     2.285205, Tokens per Sec:     5411, Lr: 0.000031\r\n",
      "2022-08-02 21:15:06,772 - INFO - joeynmt.training - Epoch  45, Step:   492200, Batch Loss:     2.129828, Tokens per Sec:     5541, Lr: 0.000031\r\n",
      "2022-08-02 21:15:25,194 - INFO - joeynmt.training - Epoch  45, Step:   492300, Batch Loss:     2.312668, Tokens per Sec:     5437, Lr: 0.000031\r\n",
      "2022-08-02 21:15:43,712 - INFO - joeynmt.training - Epoch  45, Step:   492400, Batch Loss:     2.170415, Tokens per Sec:     5552, Lr: 0.000031\r\n",
      "2022-08-02 21:16:02,139 - INFO - joeynmt.training - Epoch  45, Step:   492500, Batch Loss:     2.375344, Tokens per Sec:     5536, Lr: 0.000031\r\n",
      "2022-08-02 21:16:09,900 - INFO - joeynmt.training - Epoch  45: total training loss 1880.58\r\n",
      "2022-08-02 21:16:09,901 - INFO - joeynmt.training - Training ended after  45 epochs.\r\n",
      "2022-08-02 21:16:09,901 - INFO - joeynmt.training - Best validation result (greedy) at step   452000: 5308.08 loss.\r\n",
      "2022-08-02 21:16:10,187 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\r\n",
      "2022-08-02 21:16:10,188 - INFO - joeynmt.prediction - Loading model from models/deen_transformer/452000.ckpt\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n",
      "    \"__main__\", mod_spec)\r\n",
      "  File \"/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/runpy.py\", line 85, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 48, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/__main__.py\", line 35, in main\r\n",
      "    train(cfg_file=args.config_path, skip_test=args.skip_test)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/training.py\", line 865, in train\r\n",
      "    datasets=datasets_to_test)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/prediction.py\", line 321, in test\r\n",
      "    model_checkpoint = load_checkpoint(ckpt, use_cuda=use_cuda)\r\n",
      "  File \"/home/s3886026/NLP_thesis/JoeyNMT/joeynmt/joeynmt/helpers.py\", line 284, in load_checkpoint\r\n",
      "    assert os.path.isfile(path), f\"Checkpoint {path} not found\"\r\n",
      "AssertionError: Checkpoint models/deen_transformer/452000.ckpt not found\r\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
    "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 21:17:16,021 - INFO - root - Hello! This is Joey-NMT (version 1.5.1).\r\n",
      "2022-08-02 21:17:16,029 - INFO - joeynmt.data - Building vocabulary...\r\n",
      "2022-08-02 21:17:30,785 - INFO - joeynmt.data - Loading dev data...\r\n",
      "2022-08-02 21:17:30,975 - INFO - joeynmt.data - Loading test data...\r\n",
      "2022-08-02 21:17:31,058 - INFO - joeynmt.data - Data loaded.\r\n",
      "2022-08-02 21:17:31,073 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\r\n",
      "2022-08-02 21:17:31,073 - INFO - joeynmt.prediction - Loading model from models/deen_transformer/478000.ckpt\r\n",
      "2022-08-02 21:17:36,496 - INFO - joeynmt.model - Building an encoder-decoder model...\r\n",
      "2022-08-02 21:17:38,047 - INFO - joeynmt.model - Enc-dec model built.\r\n",
      "2022-08-02 21:17:38,320 - INFO - joeynmt.prediction - Decoding on dev set (data/deen/dev.bpe.en)...\r\n",
      "2022-08-02 21:18:03,723 - INFO - joeynmt.prediction -  dev bleu[none]:   4.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\r\n",
      "2022-08-02 21:18:03,729 - INFO - joeynmt.prediction - Translations saved to: models/deen_transformer/predictions.dev\r\n",
      "2022-08-02 21:18:03,729 - INFO - joeynmt.prediction - Decoding on test set (data/deen/test.bpe.en)...\r\n",
      "2022-08-02 21:18:32,894 - INFO - joeynmt.prediction - test bleu[none]:   2.24 [Beam search decoding with beam size = 5 and alpha = 1.0]\r\n",
      "2022-08-02 21:18:32,900 - INFO - joeynmt.prediction - Translations saved to: models/deen_transformer/predictions.test\r\n"
     ]
    }
   ],
   "source": [
    "# !cd joeynmt; python3 -m joeynmt test models/deen_transformer/config.yaml --output_path models/deen_transformer/predictions\n",
    "!cd joeynmt; python3 -m joeynmt test models/deen_transformer/config.yaml --output_path models/deen_transformer/predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jnmt_env",
   "language": "python",
   "name": "jnmt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
